<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Adam Li - Academic</title><link href="/" rel="alternate"></link><link href="/feeds/academic.atom.xml" rel="self"></link><id>/</id><updated>2017-09-25T00:00:00-04:00</updated><entry><title>Important Papers for Fundamentals in Computational Neuroscience / Data Science</title><link href="/blog/2017/09/fundamental-papers/" rel="alternate"></link><published>2017-09-25T00:00:00-04:00</published><updated>2017-09-25T00:00:00-04:00</updated><author><name>Adam Li</name></author><id>tag:None,2017-09-25:/blog/2017/09/fundamental-papers/</id><summary type="html">&lt;p&gt;To keep a log of important papers I read about and how they are relevant.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Papers&lt;/h1&gt;
&lt;h2&gt;1. Wilson-Cowan Neural Mass Model&lt;/h2&gt;
&lt;h3&gt;Summary / Conclusions:&lt;/h3&gt;
&lt;h3&gt;Important Notes:&lt;/h3&gt;
&lt;h2&gt;2. Kalman Filter Model&lt;/h2&gt;
&lt;h3&gt;Summary / Conclusions:&lt;/h3&gt;
&lt;h3&gt;Important Notes:&lt;/h3&gt;
&lt;h2&gt;3. Expectation Maximization&lt;/h2&gt;
&lt;h3&gt;Summary / Conclusions:&lt;/h3&gt;
&lt;h3&gt;Important Notes:&lt;/h3&gt;
&lt;h2&gt;4. Information Bottleneck&lt;/h2&gt;
&lt;h3&gt;Summary / Conclusions:&lt;/h3&gt;
&lt;h3&gt;Important Notes:&lt;/h3&gt;</content><category term="phd"></category><category term="journals"></category><category term="reviews"></category></entry><entry><title>Introduction to EEG Data Analysis</title><link href="/blog/2017/09/primer-eeg-analysis/" rel="alternate"></link><published>2017-09-18T00:00:00-04:00</published><updated>2017-09-18T00:00:00-04:00</updated><author><name>Adam Li</name></author><id>tag:None,2017-09-18:/blog/2017/09/primer-eeg-analysis/</id><summary type="html">&lt;p&gt;To guide the introduction of EEG analysis.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Background&lt;/h1&gt;
&lt;h1&gt;Data &amp;amp; Metadata&lt;/h1&gt;
&lt;h1&gt;Implementation&lt;/h1&gt;</content><category term="data analysis"></category><category term="eeg"></category><category term="phd"></category></entry><entry><title>Doctoral Board Oral Exam (PhD)</title><link href="/blog/2017/08/doctoral-board-oral/" rel="alternate"></link><published>2017-08-05T00:00:00-04:00</published><updated>2017-08-05T00:00:00-04:00</updated><author><name>Adam Li</name></author><id>tag:None,2017-08-05:/blog/2017/08/doctoral-board-oral/</id><summary type="html">&lt;p&gt;A short walkthrough of my experience with the DBO exam at Johns Hopkins University&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Doctoral Board Oral Exam&lt;/h1&gt;
&lt;!-- MarkdownTOC --&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Cable Theory and Compartmental Modeling&lt;/li&gt;
&lt;li&gt;Examples of different types of neurons&lt;/li&gt;
&lt;li&gt;Approach&lt;/li&gt;
&lt;li&gt;Important Equations&lt;/li&gt;
&lt;li&gt;Study&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Generalized Linear Models:&lt;/li&gt;
&lt;li&gt;General Form of Exponential Family Distribution&lt;/li&gt;
&lt;li&gt;Normal Linear Regression&lt;/li&gt;
&lt;li&gt;Logistic Regression&lt;/li&gt;
&lt;li&gt;Poisson Regression&lt;/li&gt;
&lt;li&gt;Solving GLM Methods:&lt;/li&gt;
&lt;li&gt;Goodness of Fit&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Kalman Filter&lt;/li&gt;
&lt;li&gt;Stochastic State-Space Model&lt;/li&gt;
&lt;li&gt;Assumptions&lt;/li&gt;
&lt;li&gt;Derivation&lt;/li&gt;
&lt;li&gt;Relation to a Least-Squares Problem&lt;/li&gt;
&lt;li&gt;Relation to a Bayesian Maximum Aposteri Estimation&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Expectation Maximization&lt;/li&gt;
&lt;li&gt;Basic Idea&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;K-Means Algorithm&lt;/li&gt;
&lt;li&gt;Cost Function&lt;/li&gt;
&lt;li&gt;The Algorithm&lt;/li&gt;
&lt;li&gt;Initialization&lt;/li&gt;
&lt;li&gt;Convergence&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Sensory Pathways and Systems in Neuroscience&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Motor Pathways and Systems in Neuroscience&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;General Systems in Neuroscience&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Gaussian Mixture Models&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;FAQ&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;h1&gt;1. Cable Theory and Compartmental Modeling&lt;/h1&gt;
&lt;h2&gt;Examples of different types of neurons&lt;/h2&gt;
&lt;p&gt;There are thalamic cells, pyramidal cells, double pyramidal cells, granule cells, purkinje cells. These all have different morphologies and perform different computations.&lt;/p&gt;
&lt;h2&gt;Approach&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Approximate dendrites as uniform membrane cylandiers&lt;/li&gt;
&lt;li&gt;Synaptic inputs are approximated as 'injected currents'&lt;/li&gt;
&lt;li&gt;Use the cable equation to create a system of differential equations for each cylinder.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Important Equations&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;input conductance of semi-infinite cable&lt;/li&gt;
&lt;li&gt;input conductance of infinite cable&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Unsealed end:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;cable equation&lt;/li&gt;
&lt;li&gt;input conductance of finite cable&lt;/li&gt;
&lt;li&gt;s.s. voltage along cable&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Sealed end:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;input conductance of the finite cable&lt;/li&gt;
&lt;li&gt;s.s. voltage along cable&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Concatenated Cables:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Compute V(X) along branches, by determining &lt;span class="math"&gt;\(G_{out}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Definitions:&lt;/h3&gt;
&lt;p&gt;Know definitions and related biology of:
- axial resistance, the resistance to current flow along the uniform cable (along the dendrite)
- membrane resistance, the resistance of current flow out of the dendrite
- membrance capacitance, the capacitance of a patch of membrane surface area
- derive cable equation as a model of concatenated RC circuits&lt;/p&gt;
&lt;h3&gt;Compartmental Models:&lt;/h3&gt;
&lt;p&gt;Be able to derive system of equations into a matrix form for a linear time-invariant system &lt;/p&gt;
&lt;p&gt;Understand how transfer resistances work. Understand how distributing synaptic inputs works. &lt;/p&gt;
&lt;p&gt;Understand how coincidence detection (AND operator), shunting inhibition (AND-NOT operator).&lt;/p&gt;
&lt;h2&gt;Study&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Definitions of cable theory and biological relations&lt;/li&gt;
&lt;li&gt;Solving single cable equation and different boundary conditions&lt;/li&gt;
&lt;li&gt;Derivation of cable equation under different boundary conditions&lt;/li&gt;
&lt;li&gt;Deriving LTI system from compartmental models of cables&lt;/li&gt;
&lt;li&gt;Derive transfer resistances&lt;/li&gt;
&lt;li&gt;Derive AND operator (coincidence detection)&lt;/li&gt;
&lt;li&gt;Derive AND-NOT operator (shunting inhibition)&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;2. Generalized Linear Models:&lt;/h1&gt;
&lt;h2&gt;General Form of Exponential Family Distribution&lt;/h2&gt;
&lt;p&gt;Be able to define all the terms, such as: natural paramters, sufficient statistic, natural link function, dispersion parameter&lt;/p&gt;
&lt;h2&gt;Normal Linear Regression&lt;/h2&gt;
&lt;h2&gt;Logistic Regression&lt;/h2&gt;
&lt;h2&gt;Poisson Regression&lt;/h2&gt;
&lt;h2&gt;Solving GLM Methods:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Penalized Quasi-likelihood&lt;/li&gt;
&lt;li&gt;Laplace's Method&lt;/li&gt;
&lt;li&gt;Adaptive Gaussian Quadrature&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Goodness of Fit&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Chi-square test&lt;/li&gt;
&lt;li&gt;Kolmogorov Statistic&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;3. Kalman Filter&lt;/h1&gt;
&lt;h2&gt;Stochastic State-Space Model&lt;/h2&gt;
&lt;p&gt;Here, list the linear, time-invariant system with a state evolution equation and measurement/observation equation.&lt;/p&gt;
&lt;h2&gt;Assumptions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;state and observation noises are independent, zero-mean Gaussian white processes with some defined covariances&lt;/li&gt;
&lt;li&gt;initial state x_0 is a Gaussian R.V. independent of the state/observation noises&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Derivation&lt;/h2&gt;
&lt;p&gt;Derive the Kalman filter equations for the state update, covarariance estimates&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Measurement update&lt;/li&gt;
&lt;li&gt;Time update&lt;/li&gt;
&lt;li&gt;Combined Update&lt;/li&gt;
&lt;li&gt;Covariance Update&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Relation to a Least-Squares Problem&lt;/h2&gt;
&lt;h2&gt;Relation to a Bayesian Maximum Aposteri Estimation&lt;/h2&gt;
&lt;h1&gt;4. Expectation Maximization&lt;/h1&gt;
&lt;h2&gt;Basic Idea&lt;/h2&gt;
&lt;p&gt;By computing the likelihood of your unknown parameters, based on known outcomes. You can perform maximum likelihood estimation to get the best estimate of the unknown parameters&lt;/p&gt;
&lt;h1&gt;5. K-Means Algorithm&lt;/h1&gt;
&lt;h2&gt;Cost Function&lt;/h2&gt;
&lt;p&gt;The cost function is attempting to minimize the distortion (distance to centers) for every point in the set of data points, S.&lt;/p&gt;
&lt;h2&gt;The Algorithm&lt;/h2&gt;
&lt;p&gt;'''
Input: k clusters
initialize centers: z_1, ..., z_k \in \real^d and clusters C_1, ..., C_k
repeat until there is no further change in L(z, C):
    for each j (data point): C_j &amp;lt;- {x \in S whose closest center is z_j}
    for each j (data point): z_j &amp;lt;- mean(C_j) 
'''&lt;/p&gt;
&lt;p&gt;Walk through for a small example to see how the algorithm works:
(0, 0, 0)
(0, 0.5, 1)
(1, 3, 2)
(4, 5, 6)
(2, 3, 1)
(5, 2, 0)
(0, 1, 0)
(1, 1, 0)
(2, 1, 0)&lt;/p&gt;
&lt;h2&gt;Initialization&lt;/h2&gt;
&lt;h2&gt;Convergence&lt;/h2&gt;
&lt;p&gt;Show that the cost monotonically decreases, so the algorithm will converge at least in the sense of cost decreasing to a non-changing amount.&lt;/p&gt;
&lt;h1&gt;6. Sensory Pathways and Systems in Neuroscience&lt;/h1&gt;
&lt;h1&gt;7. Motor Pathways and Systems in Neuroscience&lt;/h1&gt;
&lt;h1&gt;8. General Systems in Neuroscience&lt;/h1&gt;
&lt;h1&gt;9. Gaussian Mixture Models&lt;/h1&gt;
&lt;h1&gt;FAQ&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Q: Where should I stay? 
A: Generally, I'm an advocate of staying downtown because then everything is walking distance. If not, I would try to stay within walking distance to a subway station. Stay away from South Chicago because that is a very crime-ridden area.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Q: I only have a day in Seoul, where should I go?
A: Purple pig, Girl on the goat, Lou Malnati's to eat. Cloud Gate / Millenium Park and the River Walk to see. Signature Room for a night time drink if you have the time.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="doctoral board oral"></category><category term="phd"></category><category term="johns hopkins"></category></entry></feed>