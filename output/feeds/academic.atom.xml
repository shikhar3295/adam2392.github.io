<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Adam Li - Academic</title><link href="/" rel="alternate"></link><link href="/feeds/academic.atom.xml" rel="self"></link><id>/</id><updated>2018-06-01T00:00:00-04:00</updated><entry><title>Using The Virtual Brain to Understand Algorithms</title><link href="/blog/2018/06/whitaker-summary-experience/" rel="alternate"></link><published>2018-06-01T00:00:00-04:00</published><updated>2018-06-01T00:00:00-04:00</updated><author><name>Adam Li</name></author><id>tag:None,2018-06-01:/blog/2018/06/whitaker-summary-experience/</id><summary type="html">&lt;p&gt;To summarize my Whitaker/Chateaubriand research experience abroad in Marseille, France.&lt;/p&gt;</summary><content type="html">&lt;!-- MarkdownTOC --&gt;

&lt;ul&gt;
&lt;li&gt;Background&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Epilepsy&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Computational Modeling&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Algorithms&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Random Notes&lt;ul&gt;
&lt;li&gt;Data Pipeline Design&lt;/li&gt;
&lt;li&gt;PhD and Research Understanding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Concepts&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;TVB vs Network Data Analysis&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Using TVB To Augment Neural Datasets For Deep Learning&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Conclusions / Future Considerations
        - References:&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;h1&gt;Background&lt;/h1&gt;
&lt;h2&gt;1. Epilepsy&lt;/h2&gt;
&lt;p&gt;Epilepsy is a disease that affects more then 70 M people worldwide, which characterizes itself with seizures (i.e. abnormal brain activity) for seconds to several minutes. Epilepsy can be treated with medicine that generally inhibits brain activity (albeit with numerous side effects), and also with surgery. Surgery can involve resection (i.e. cutting a portion of the hypothesized diseased brain) and laser ablation (i.e. heat treatment of small spherical regions within the brain). When successful, surgery can result in complete seizure freedom!&lt;/p&gt;
&lt;p&gt;However, the problem is that surgery is extremely variable in success (e.g. ~50% average), even though mortality rates have drastically lowered. A main obstacle to high success rates is incorrect localization of the epileptogenic zone, the clinical region of the brain that seizures originate from. The goal of any researcher in this field is to identify biomarkers and methods for robustly identifying this diseased brain region given neural data. Neural data can come in the form of electrophysiological recordings, MRI images, Diffusion weighted MRI images and CT images.&lt;/p&gt;
&lt;h2&gt;2. Computational Modeling&lt;/h2&gt;
&lt;p&gt;Computational modeling is the art of using mathematical equations to model how certain systems (i.e. the brain) behaves given parameters you input. So here, at Marseille, the group has developed computational models that take in the patient's specific reconstructed brain tractography and geometry to model epilepsy. The way it does this is by parcellating the brain into multiple regions, where each region is modeled by a computational model that can exhibit seizure phenomena. By coupling every region based on realistic brain connectivity, you can simulate brain behavior when you "set" different regions of the brain to be "diseased". One could then compare the simulation and the real electrophysiological recording data in patients to test different hypotheses, such as: 
1. what happens if we set this region to be epileptogenic? does it resemble the real dynamics recorded in the patient's brain? 
2. If we remove this region of the brain (i.e. remove connectivity to and from this region), will it help prevent propagation of seizure activity to healthy regions of the brain?
3. Can we generate realistic data that can reflect feature variability of the real recording data?&lt;/p&gt;
&lt;h2&gt;3. Algorithms&lt;/h2&gt;
&lt;p&gt;Algorithms are spelling out a certain set of computations that a program should undertake to get a specific answer. In our research group, we are attempting to develop algorithms that take in brain data of the patient and outputs a prediction of the diseased region. There are different kinds of algorithms that use the data in various ways. In general, all data analysis tries to represent data in various ways. Does the amplitude, or frequency of the data matter? Should we represent the data as a graph? Should we apply filters to the data to remove noise? Should we just leave the data alone and let the model decide what is important?&lt;/p&gt;
&lt;p&gt;Our research group currently developed a linear algorithm that analyzes the data as a graph by applying a very specific type of computation. Namely, it constructs a graph out of the dataset and applies perturbations to the graph (i.e. add's vectors of noise in a very structured way), to determine which regions of the brain are most susceptible to being perturbed into seizuring. This specific type of analysis requires some simple linear systems theory to derive an analytical equation for doing this.&lt;/p&gt;
&lt;p&gt;On the other end of the spectrum, one could apply deep learning to this problem for a way of supervised learning. What this would require is knowing the exact regions of the brain that are diseased and then feeding in the data and the labels of the regions to let the model determine which features of the data are most predictive of the diseased region. This is more general, but can require large amounts of data and hyperparameter tuning of the neural networks.&lt;/p&gt;
&lt;h1&gt;Random Notes&lt;/h1&gt;
&lt;p&gt;This year, I really had to deal with more data then I was accustomed to at JHU. At JHU, I had access to various text files, EEG recordings and MRI/CT imaging data for various patients from various clinical centers. However, these recordings were never longer then 5-10 minutes. &lt;/p&gt;
&lt;p&gt;Here, I had to deal with data at scale. I had to understand how to optimize parallelized runs of linear algorithms on said data. My datasets went from a couple hundred MB (i.e. few recordings for a single patient), to a few GB (i.e. multiple patients) to a couple hundred GB and few TBs (i.e. &amp;gt;50 patients with multiple recordings). My analysis and data pipeline design scaled at the same time, but required me to refactor and understand how to continuously analyze the data robustly and efficiently.&lt;/p&gt;
&lt;p&gt;I started out with analyzing datasets on my laptop/workstation with unoptimized code. Then I proceeded to optimize different parts of my workflow and moduralize it, so that it could be parallelized onto multiple cores. I then proceeded to submit it onto the JHU High-Performance Computing cluster. &lt;/p&gt;
&lt;h2&gt;Data Pipeline Design&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;First, I began with one CPU per window of data (each dataset was split into a number of windows to analyze). This resulted in hundreds-thousands of CPUs being fired up with loading the data and then analyzing a short segment of the dataset. This had numerous problems. One CPU out of hundreds/thousands could easily fail the run the job correctly, which would result in a missing computed window.&lt;/li&gt;
&lt;li&gt;Then, I began using a parallel framework with GNU that ran each dataset with a fixed 24 cores per node. This helped because GNU allows you to restart jobs using a log file, but this was also problematic because as datasets grew longer, it wasn't clear how long I would have to wait to get data per each patient.&lt;/li&gt;
&lt;li&gt;Now, I also break up datasets into chunks and then analyze using 24-48 cores at a time that sift through the data. These data analyzed-chunks can later be combined to form into the one dataset if necessary, but a metadata json object is used to store information allowing anyone using the data to understand how to put computations together.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;PhD and Research Understanding&lt;/h2&gt;
&lt;p&gt;A lot of this year was spent reading papers and doing a lot of thinking... Not very productive, but I really felt like I learned a lot and expanded my research mindset.&lt;/p&gt;
&lt;p&gt;Couple things I started learning more about:&lt;/p&gt;
&lt;h1&gt;Concepts&lt;/h1&gt;
&lt;h2&gt;1. TVB vs Network Data Analysis&lt;/h2&gt;
&lt;p&gt;This was the main project proposed when I applied for the Whitaker/Chateaubriand fellowships. The goal was to use the flexible modeling capabilities of "The Virtual Brain" platform developed here in Marseille to understand how network data predictions of the epileptogenic zone performs under various model configurations. So, how can our predictions work under various clinical settings? Can we arrive at the same conclusion when our algorithm is applied to an in-silico model?&lt;/p&gt;
&lt;p&gt;Being able to demonstrate agreement by using this whole-brain model helps provide evidence on two fronts: 1. provides additional evidence that TVB is capable of modeling the dynamical characteristics of seizures realistically and 2. provide hypothetical constraints on data analysis by providing ground-truth simulations of epileptic seizures and demonstrating when the algorithms work.&lt;/p&gt;
&lt;h2&gt;2. Using TVB To Augment Neural Datasets For Deep Learning&lt;/h2&gt;
&lt;p&gt;A core problem of deep learning is the amount of data required to train successful models to perform classification/regression. This is especially apparent in models surrounding neuroscience and neural data. That is because neural data is traditionally difficult to obtain, and is extremely constrained because of the different variables that go into collecting the data. For this reason, recording data from one clinical center can be significantly different from another clinical center because of protocols used and hardware settings.&lt;/p&gt;
&lt;p&gt;TVB at its core is a generative model of the brain that utilizes realistic brain geometry, connectivity and clinical hypotheses to simulate electrophysiological signals. Based on the model placed into TVB, it can generate different patterns of behavior, or in our case, epileptic seizures. This is extremely important because as a scientist, you can control the variables to produce different types of data, but all demonstrating epileptic seizing. This hypothetically would give you a huge amount of variable data, that has ground truth set by you, and also only require computing power to simulate. It is not constrained by medical procedures and could help in generating data that deep learning models can learn from for each patient BEFORE surgery. This could help establish a completely in-silico pipeline for helping clinicians make informed decisions and hypotheses before the patient is operated on.&lt;/p&gt;
&lt;h1&gt;Conclusions / Future Considerations&lt;/h1&gt;
&lt;h3&gt;References:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;http://www.thevirtualbrain.org/tvb/&lt;/li&gt;
&lt;li&gt;https://ieeexplore.ieee.org/document/7963378/&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="tvb"></category><category term="phd"></category></entry><entry><title>Using FreeSurfer</title><link href="/blog/2017/12/using-freesurfer/" rel="alternate"></link><published>2017-12-07T00:00:00-05:00</published><updated>2017-12-07T00:00:00-05:00</updated><author><name>Adam Li</name></author><id>tag:None,2017-12-07:/blog/2017/12/using-freesurfer/</id><summary type="html">&lt;p&gt;To guide the user in how to setup freesurfer correctly.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Questions&lt;/h1&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;Freesurfer is a tool built for rendering 3D brains using MRI and Ct scans.&lt;/p&gt;
&lt;p&gt;FSL is a tool for coregistration and image analysis.&lt;/p&gt;
&lt;p&gt;Download both online:&lt;/p&gt;
&lt;h2&gt;Common Definitions:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Registration: to find a common coordinate system for the input data sets&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Implementation&lt;/h1&gt;
&lt;h2&gt;1. Set Up&lt;/h2&gt;
&lt;p&gt;First you will want to download freesurfer from the following website:&lt;/p&gt;
&lt;p&gt;https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall
Linux:
    ## bash
    $&amp;gt; export FREESURFER_HOME=/usr/local/freesurfer
    $&amp;gt; source $FREESURFER_HOME/SetUpFreeSurfer.sh&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;## tcsh
$&amp;gt; setenv FREESURFER_HOME /usr/local/freesurfer
$&amp;gt; source $FREESURFER_HOME/SetUpFreeSurfer.csh
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Mac:
    $&amp;gt; export FREESURFER_HOME=/Applications/freesurfer
    $&amp;gt; source $FREESURFER_HOME/SetUpFreeSurfer.sh&lt;/p&gt;
&lt;p&gt;Run those commands within your terminal, or add them to ~/.bashrc file to have access to all the command line tools for freeview.&lt;/p&gt;
&lt;p&gt;You will need to setup your own directory where you will hold all your subject data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;export SUBJECTS_DIR=&amp;lt;path to subject data&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This will be where you store say patient's MRI/CT scans that you will use to reconstruct the brain.&lt;/p&gt;
&lt;h2&gt;2. Running Through T1-Weighted MRI Images&lt;/h2&gt;
&lt;p&gt;First you want to set your current Subjects directory to where you are working with the raw say .dcm data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;export SUBJECTS_DIR=$PWD
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;There may be an issue where you can't run the functions with your .dcm files. I have no idea why this occurs, but an easy fix is to externally run a dicom to nii converter and pass this type of file instead. Here is a link to a matlab converter that can do this:
https://www.mathworks.com/matlabcentral/fileexchange/42997-dicom-to-nifti-converter--nifti-tool-and-viewer&lt;/p&gt;
&lt;p&gt;There are also other resources online.&lt;/p&gt;
&lt;p&gt;Afterwards, you can run the following command(s):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;recon-all -i &amp;lt;data&amp;gt;.nii -s &amp;lt;subject_name&amp;gt; -autorecon1
recon-all -i &amp;lt;data&amp;gt;.nii -s &amp;lt;subject_name&amp;gt; -all
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This will take a long time! So be prepared to run this on a compute engine that has time.&lt;/p&gt;
&lt;h2&gt;3. Running Through CT Images&lt;/h2&gt;
&lt;p&gt;flirt -in patient_ct.nii -ref patient_mri.nii -omat patient_omat.mat -out patient_registered.nii.gz&lt;/p&gt;
&lt;p&gt;This command will coregister the CT data onto the domain of the MRI data and provide a coregistration for you to look at where certain contacts of electrodes are. You can also view in Freeview the different cuts of the brain using either CT, or MRI. &lt;/p&gt;
&lt;h2&gt;4. Getting Surface Parcellations&lt;/h2&gt;
&lt;h2&gt;5. Getting SEEG XYZ Coordinates&lt;/h2&gt;
&lt;p&gt;Once you have CT images coregistered with MRI images, you can easily extract the locations of all iEEG contacts in the MRI axis system. This is done by noting a contact within each electrode (e.g. A1, B1, C10, etc.) and then an algorithm can fill in the entire electrode's xyz coordinates and output to a file.&lt;/p&gt;
&lt;h1&gt;References:&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall#Setup.26Configuration&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="data analysis"></category><category term="eeg"></category><category term="phd"></category><category term="brain rendering"></category></entry><entry><title>Important Concepts for Computational Modeling</title><link href="/blog/2017/09/fundamental-computational-modeling/" rel="alternate"></link><published>2017-09-27T00:00:00-04:00</published><updated>2017-09-27T00:00:00-04:00</updated><author><name>Adam Li</name></author><id>tag:None,2017-09-27:/blog/2017/09/fundamental-computational-modeling/</id><summary type="html">&lt;p&gt;To keep a log of important concepts in computational modeling.&lt;/p&gt;</summary><content type="html">&lt;!-- MarkdownTOC --&gt;

&lt;ul&gt;
&lt;li&gt;Background&lt;/li&gt;
&lt;li&gt;Concepts&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Numerical Integration&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Deterministic and Stochastic Differential Equations&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Deterministic vs Stochastic&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;Computational modeling involves setting up mathematical equations that represent some sort of statics, or dynamics within a system. You want to simulate these equations to understand behavior, fixed points, bifurcations and changes wrt parameter values.&lt;/p&gt;
&lt;h1&gt;Concepts&lt;/h1&gt;
&lt;h2&gt;1. Numerical Integration&lt;/h2&gt;
&lt;p&gt;Numerical integration involves solving system of differential equations via numerical methods. This is in general split into deterministic vs stochastic differential equations. Here we will also split the two.&lt;/p&gt;
&lt;h3&gt;Deterministic Differential Equations&lt;/h3&gt;
&lt;p&gt;1) Euler's Method&lt;/p&gt;
&lt;p&gt;2) Runge-Kutta Method(s)?&lt;/p&gt;
&lt;h3&gt;Stochastic Differential Equations&lt;/h3&gt;
&lt;p&gt;Heun methods seem to be more accurate, but more time consuming vs the Milstein method.
1) Heun Method
Simple discretization leading to Stratonovich integral. It is a "predictor-corrector method" because given value of X at time &lt;span class="math"&gt;\(t_n\)&lt;/span&gt;, obtain predictors with Euler integration scheme, then correct it using Heun's correction.&lt;/p&gt;
&lt;p&gt;Details to be filled in.&lt;/p&gt;
&lt;p&gt;2) Milstein Method
Uses the derivative of the diffusion coefficients&lt;/p&gt;
&lt;h3&gt;Considerations:&lt;/h3&gt;
&lt;p&gt;1) Convergence&lt;/p&gt;
&lt;h3&gt;References:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Comparison of stochastic integration https://arxiv.org/pdf/1102.4401.pdf&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;2. Deterministic and Stochastic Differential Equations&lt;/h2&gt;
&lt;h3&gt;Deterministic Differential Equations&lt;/h3&gt;
&lt;p&gt;An example would be a population growth model. &lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(dx_t = Kx_t dt,\ x(0)=x_0\)&lt;/span&gt;, with K being some constant.&lt;/p&gt;
&lt;p&gt;However, if we have some inherent randomness, then maybe we can't assume &lt;span class="math"&gt;\(x_0\)&lt;/span&gt; is deterministic constant, but perhaps a random variable.&lt;/p&gt;
&lt;h3&gt;Stochastic Differential Equations&lt;/h3&gt;
&lt;p&gt;An example would be a population growth model. &lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(dX_t(w) = KX_t(w) dt,\ X_0(w)\)&lt;/span&gt;, with K being some constant and &lt;span class="math"&gt;\(X_t(w)\)&lt;/span&gt; is a random variable, which comes from the initial condition. K could also be random.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(dX_t(w) = (Kdt + dW_t(w))X_t(w) dt,\ X_0(w)\)&lt;/span&gt;, where &lt;span class="math"&gt;\(dW_t(w)\)&lt;/span&gt; is some noise process that adds randomness to K.&lt;/p&gt;
&lt;p&gt;This leads us to the general SDE. &lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(dX_t(w) = f_t(X_t(w))dt + \sigma_t(X_t(w))dW_t(w),\ X_0(w)\)&lt;/span&gt;, where f is the deterministic drift of the SDE. &lt;span class="math"&gt;\(\sigma_t\)&lt;/span&gt; is the diffusion coefficient, and &lt;span class="math"&gt;\(dW_t(w)\)&lt;/span&gt; is the noise process. &lt;/p&gt;
&lt;p&gt;We can rewrite the SDE in integral form, which leads us to the Stratonovich integral and the Ito integral.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(X_t(w) = X_0(w) + \int_t_0 f_s(X_s(w)) ds + \int_t_0 \sigma_s(X_s(w)) dW_s(w)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Ito integral =&amp;gt;
Stratonovich integral =&amp;gt;&lt;/p&gt;
&lt;h3&gt;References:&lt;/h3&gt;
&lt;h2&gt;1. Deterministic vs Stochastic&lt;/h2&gt;
&lt;h3&gt;References:&lt;/h3&gt;
&lt;h3&gt;Summary / Conclusions:&lt;/h3&gt;
&lt;h3&gt;Important Notes:&lt;/h3&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="phd"></category><category term="journals"></category><category term="reviews"></category><category term="computational modeling"></category></entry><entry><title>Simulating Epileptic iEEG Activity Using The Virtual Brain</title><link href="/blog/2017/09/simulating-tvb/" rel="alternate"></link><published>2017-09-27T00:00:00-04:00</published><updated>2017-09-27T00:00:00-04:00</updated><author><name>Adam Li</name></author><id>tag:None,2017-09-27:/blog/2017/09/simulating-tvb/</id><summary type="html">&lt;p&gt;To guide the simulation of Epileptic iEEG activity using TVB in Marseille, France.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Questions&lt;/h1&gt;
&lt;p&gt;Using convolutional and recurrent neural networks, this paper showed that they could perform seizure detection as well as incidence detection (of some experimental marker). I am interested in the possibility of exploring how this algorithm might perform on simulated TVB data, which to my knowledge has not been done yet. I would be interested in seeing how this improves, or is similar to training on the real data.&lt;/p&gt;
&lt;p&gt;Then I was going to see if I could extend this same network onto the fragility maps.&lt;/p&gt;
&lt;p&gt;I was wondering:
1. Would it be possible to get access to patient's SEEG data that has the SEEG xyz coordinates, and raw iEEG data? (n as big as can be)
2. A problem with the previous paper was the unbalance of data in training (e.g. much less seizure starts, then non seizure instances). For TVB, is it possible to virtually ensure seizures happen at a certain rate if I simulated for say... 2 minutes, 100 times? So that we could have many instances of seizure occurence and could train on an equal set of seizure occurence and non seizure?
3. Can TVB determine when the seizure occurred exactly in Epileptor time series?&lt;/p&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;TVB is a platform for simulating whole-brain dynamics that starts from raw data involving:
    1. structural connectivity derived from DTI
    2. brain parcellation derived from MRI and CT
    3. SEEG xyz locations derived from MRI and CT
This will then determine a gain matrix to determine SEEG signals from the source signals that are generated from neural mass models.&lt;/p&gt;
&lt;p&gt;The neural mass models will be implemented with nonlinear, complex models for simulating certain type of electrophysiology. The Epileptor is used for simulating seizure activity from a specific source region. &lt;/p&gt;
&lt;p&gt;The epileptor is a set of coupled differential equations that rely on 6 different variables. They are described here:&lt;/p&gt;
&lt;h1&gt;Data &amp;amp; Metadata&lt;/h1&gt;
&lt;p&gt;The minimum necessary requirements for creating the TVB dataset are a set of T1 and DWI images as a list of dicom files, or a single 4-D image nifti file.&lt;/p&gt;
&lt;p&gt;A high level summary of how the pipeline proceeds is:
1. Construct Cortical Surface, Subcortical Surface
Using freesurfer, you can get the reconstructed surfaces, which are your files that outline the voxels that belong to each region of the brain.
2. Construct Parcellation Scheme
This can range from the default in freesurfer to different atlases available for the human brain.
3. Construct Corticography Tracts
First, you need to coregister the DWI images with the T1 scans&lt;/p&gt;
&lt;p&gt;Using the DWI images, along with the reconstructed surfaces, you can count fiber tracts between each region of the brain and reconstruct the structural connectivity matrices. This is composed from the weights matrix and the length matrix between parcellated regions.
4. Obtain Electrode Coordinates in T1 Space
First, you need to coregister the CT reconstructed freesurfer file into the T1 space. &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Implementation&lt;/h1&gt;
&lt;h2&gt;1. Setting Up Environment&lt;/h2&gt;
&lt;p&gt;First you may want to set up a conda environment, or a virtualenv that will separate the entire python project from your normal OS.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$
pip install nibabel networkx
git clone https://github.com/the-virtual-brain/tvb-data
git clone https://github.com/the-virtual-brain/tvb-library
$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If you want to have a script to add these all to path for your jupyter notebook, use the following:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4
5
6
7
8
9&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/bin/bash&lt;/span&gt;
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Launching IPython Notebook from TVB Distribution&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; -z &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$LANG&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt;
    &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;LANG&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;en_US.UTF-8
&lt;span class="k"&gt;fi&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;LC_ALL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$LANG&lt;/span&gt;
&lt;span class="c1"&gt;# add tvb data and library to path and launch notebook&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;PYTHONPATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;&lt;span class="nb"&gt;pwd&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;/_tvbdata:&lt;span class="k"&gt;$(&lt;/span&gt;&lt;span class="nb"&gt;pwd&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;/_tvblibrary:&lt;span class="nv"&gt;$PYTHONPATH&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
jupyter notebook
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;h2&gt;1b. Setting Up Environment on a Cluster&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;. /soft/miniconda3/activate
conda env list
conda create -n tridesclous python=3.6 scipy numpy pandas scikit-learn matplotlib seaborn pyqt=5 ipykernel
source activate tridesclous
pip install pyqtgraph
pip install https://github.com/tridesclous/tridesclous/archive/master.zip
python -m ipykernel install --name tridesclous-testing â€”user
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;2. Simulating Epilepsy&lt;/h2&gt;
&lt;p&gt;In order to simulate epilepsy, you are going to walk through a pipeline using TVB. &lt;/p&gt;
&lt;p&gt;i. Structural Connectivity&lt;/p&gt;
&lt;p&gt;ii. Neural Mass Model&lt;/p&gt;
&lt;p&gt;iii. Integrators&lt;/p&gt;
&lt;p&gt;iv. Monitors&lt;/p&gt;
&lt;p&gt;Then once these are complete, you can run your simulation.&lt;/p&gt;</content><category term="data analysis"></category><category term="eeg"></category><category term="phd"></category><category term="computational modeling"></category></entry><entry><title>Important Papers for Fundamentals in Computational Neuroscience / Data Science</title><link href="/blog/2017/09/fundamental-papers/" rel="alternate"></link><published>2017-09-25T00:00:00-04:00</published><updated>2017-09-25T00:00:00-04:00</updated><author><name>Adam Li</name></author><id>tag:None,2017-09-25:/blog/2017/09/fundamental-papers/</id><summary type="html">&lt;p&gt;To keep a log of important papers I read about and how they are relevant.&lt;/p&gt;</summary><content type="html">&lt;!-- MarkdownTOC --&gt;

&lt;ul&gt;
&lt;li&gt;Papers&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Wilson-Cowan Neural Mass Model&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Kalman Filter Model&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Expectation Maximization&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Information Bottleneck Method&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Opening the Black Box of Deep Neural Networks Via Information&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Reinforcement Learning: An Overview&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Deep Reinforcement Learning: An Overview&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Variational Inference:&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;h1&gt;Papers&lt;/h1&gt;
&lt;h2&gt;1. Wilson-Cowan Neural Mass Model&lt;/h2&gt;
&lt;h3&gt;Summary / Conclusions:&lt;/h3&gt;
&lt;h3&gt;Important Notes:&lt;/h3&gt;
&lt;h2&gt;2. Kalman Filter Model&lt;/h2&gt;
&lt;h3&gt;Summary / Conclusions:&lt;/h3&gt;
&lt;h3&gt;Important Notes:&lt;/h3&gt;
&lt;h2&gt;3. Expectation Maximization&lt;/h2&gt;
&lt;h3&gt;Summary / Conclusions:&lt;/h3&gt;
&lt;h3&gt;Important Notes:&lt;/h3&gt;
&lt;h2&gt;4. Information Bottleneck Method&lt;/h2&gt;
&lt;p&gt;http://www.cs.huji.ac.il/labs/learning/Papers/allerton.pdf&lt;/p&gt;
&lt;h3&gt;Summary / Conclusions:&lt;/h3&gt;
&lt;p&gt;Let us define X as input signal, and Y as desired output.&lt;/p&gt;
&lt;p&gt;Here, they were interested in deriving a quantitative method for optimizing 1) compression rate of a signal and 2) the choice of representation of the original signal.&lt;/p&gt;
&lt;p&gt;Previous theory looked at minimizing the rate of compression given a constraint on expected distortion of the original signal (with new compression). This was solved via iterative algorithm (similar to EM), but lacked generality to find optimal representatives, which minimize the expected distortion (not just compression) of the signal. &lt;/p&gt;
&lt;p&gt;The new theory looks at minimizing the rate of compression given a constraint on the amount of information we can keep about Y using X. This produces an iterative algorithm that also can be solved iteratively. It also shows that 1) the Kullback-Leibler divergence is the relevant distortion measure for the information bottleneck setting, and 2) optimization of representation of signal and the signal compression can be done together.&lt;/p&gt;
&lt;p&gt;This work can be used in applications to information processing problems (e.g. deep learning).&lt;/p&gt;
&lt;h3&gt;Important Notes:&lt;/h3&gt;
&lt;p&gt;Mutual information is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(I(X;Y) = D_{KL}[P(x,y)||P(x)P(y)] = \sum_{x\in X, y\in Y} P(x,y) log(\frac_{P(x,y)}_{P(x)P(y)})\)&lt;/span&gt;
&lt;span class="math"&gt;\(= \sum_{x\in X, y\in Y}P(x,y) log(\frac_{P(x|y)}_{P(x)}) = H(X) - H(X|Y)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;, where &lt;span class="math"&gt;\(D_{KL}(p||q)\)&lt;/span&gt; is the Kullback-Liebler divergence of distributions p and q, and &lt;span class="math"&gt;\(H(X)\)&lt;/span&gt; and &lt;span class="math"&gt;\(H(X|Y)\)&lt;/span&gt; are entropy and conditional entropies, respectively.&lt;/p&gt;
&lt;p&gt;We want the optimal representations of signal X with respect to output label Y. Sufficient statistics are maps/partitions of X, S(X) that captures all the information X has about Y. The mutual information given Y is equal. &lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(I(S(X); Y) = I(X; Y)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can allow the map to be stochastic, with encoder P(T|X) and allow map to capture as much as possible of I(X;Y), not necessarily all of it &lt;span class="math"&gt;\(I(S(X); Y) \leq I(X; Y)\)&lt;/span&gt;. Define &lt;span class="math"&gt;\(t \in T\)&lt;/span&gt; as compressed representations of &lt;span class="math"&gt;\(x \in X\)&lt;/span&gt;, stochastically, &lt;span class="math"&gt;\(p(t|x)\)&lt;/span&gt;. The following optimization problem finds a balance between compression of X and prediction of Y.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(min_{p(t|x),p(y|t),p(t)}\ {I(X;T) - \beta I(T;Y)}\)&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;5. Opening the Black Box of Deep Neural Networks Via Information&lt;/h2&gt;
&lt;p&gt;Reference: https://arxiv.org/pdf/1703.00810.pdf&lt;/p&gt;
&lt;h3&gt;Summary / Conclusions:&lt;/h3&gt;
&lt;p&gt;In this paper, the authors extend their analysis of DNN using information theory. They answered the following questions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The SGD layer dynamics in the Information plane.
First the layers increase &lt;span class="math"&gt;\(I(T_i;Y)\)&lt;/span&gt;, and then later decrease &lt;span class="math"&gt;\(I(X; T_i)\)&lt;/span&gt;, which corresponds to increasing the information about Y and then later compressing the representation (empirical error minimization &amp;amp; representation compression phase).&lt;/li&gt;
&lt;li&gt;The effect of the training sample size on the layers.
It seems that sample size does not have an effect on empirical error minimization, but does have an effect on the representation compression. Smaller sample sizes has overfitting, which has been seen as overfitting the sample noise. &lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What is the benefit of the hidden layers?
Adding hidden layers reduces the number of training epochs. It also seems to accelerate compression, but adding extra width does not seem to help. With layered diffusion of the SGD optimization (backpropagation), it seems that there is an exponential decrease in epochs with K hidden layers.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What is the final location of the hidden layers?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Do the hidden layers form optimal IB representations?
It seems that the hidden layers converge to the optiaml IB representations. However, we can see that there can be clearly many different layers that correspond to the same IB representation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another important hypothesis/conjecture they make is that: attempts to interpret single weights, or even single neurons in such networks can be meaningless because there is a large number of different networks that can achieve optimal performance. This makes sense in terms of how the brain is structured; the brain does not form the same pathway for learning something new between different people, but form a unique network for optimal performance. &lt;/p&gt;
&lt;h3&gt;Important Notes:&lt;/h3&gt;
&lt;p&gt;First, they estimated the mutual information of the layers with the input and with the labels &lt;span class="math"&gt;\(I(X;T_i)\)&lt;/span&gt; and &lt;span class="math"&gt;\(I(T_i;Y)\)&lt;/span&gt;. &lt;/p&gt;
&lt;h2&gt;6. Reinforcement Learning: An Overview&lt;/h2&gt;
&lt;p&gt;Reference: https://pdfs.semanticscholar.org/b373/b0c6e3b4fef4ac0534965708fc382343f8dc.pdf&lt;/p&gt;
&lt;h3&gt;Summary / Conclusions:&lt;/h3&gt;
&lt;h3&gt;Important Notes:&lt;/h3&gt;
&lt;h2&gt;7. Deep Reinforcement Learning: An Overview&lt;/h2&gt;
&lt;p&gt;Reference:&lt;/p&gt;
&lt;h2&gt;8. Variational Inference:&lt;/h2&gt;
&lt;h3&gt;Summary / Conclusions&lt;/h3&gt;
&lt;p&gt;Variational inference&lt;/p&gt;
&lt;h3&gt;Important Notes&lt;/h3&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="phd"></category><category term="journals"></category><category term="reviews"></category></entry><entry><title>Doctoral Board Oral Exam (PhD)</title><link href="/blog/2017/08/doctoral-board-oral/" rel="alternate"></link><published>2017-08-05T00:00:00-04:00</published><updated>2017-08-05T00:00:00-04:00</updated><author><name>Adam Li</name></author><id>tag:None,2017-08-05:/blog/2017/08/doctoral-board-oral/</id><summary type="html">&lt;p&gt;A short walkthrough of my experience with the DBO exam at Johns Hopkins University&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Doctoral Board Oral Exam&lt;/h1&gt;
&lt;!-- MarkdownTOC --&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Cable Theory and Compartmental Modeling&lt;/li&gt;
&lt;li&gt;Examples of different types of neurons&lt;/li&gt;
&lt;li&gt;Approach&lt;/li&gt;
&lt;li&gt;Important Equations&lt;/li&gt;
&lt;li&gt;Study&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Generalized Linear Models:&lt;/li&gt;
&lt;li&gt;General Form of Exponential Family Distribution&lt;/li&gt;
&lt;li&gt;Normal Linear Regression&lt;/li&gt;
&lt;li&gt;Logistic Regression&lt;/li&gt;
&lt;li&gt;Poisson Regression&lt;/li&gt;
&lt;li&gt;Solving GLM Methods:&lt;/li&gt;
&lt;li&gt;Goodness of Fit&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Kalman Filter&lt;/li&gt;
&lt;li&gt;Stochastic State-Space Model&lt;/li&gt;
&lt;li&gt;Assumptions&lt;/li&gt;
&lt;li&gt;Derivation&lt;/li&gt;
&lt;li&gt;Relation to a Least-Squares Problem&lt;/li&gt;
&lt;li&gt;Relation to a Bayesian Maximum Aposteri Estimation&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Expectation Maximization&lt;/li&gt;
&lt;li&gt;Basic Idea&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;K-Means Algorithm&lt;/li&gt;
&lt;li&gt;Cost Function&lt;/li&gt;
&lt;li&gt;The Algorithm&lt;/li&gt;
&lt;li&gt;Initialization&lt;/li&gt;
&lt;li&gt;Convergence&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Sensory Pathways and Systems in Neuroscience&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Motor Pathways and Systems in Neuroscience&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;General Systems in Neuroscience&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Gaussian Mixture Models&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;FAQ&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;h1&gt;1. Cable Theory and Compartmental Modeling&lt;/h1&gt;
&lt;h2&gt;Examples of different types of neurons&lt;/h2&gt;
&lt;p&gt;There are thalamic cells, pyramidal cells, double pyramidal cells, granule cells, purkinje cells. These all have different morphologies and perform different computations.&lt;/p&gt;
&lt;h2&gt;Approach&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Approximate dendrites as uniform membrane cylandiers&lt;/li&gt;
&lt;li&gt;Synaptic inputs are approximated as 'injected currents'&lt;/li&gt;
&lt;li&gt;Use the cable equation to create a system of differential equations for each cylinder.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Important Equations&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;input conductance of semi-infinite cable&lt;/li&gt;
&lt;li&gt;input conductance of infinite cable&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Unsealed end:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;cable equation&lt;/li&gt;
&lt;li&gt;input conductance of finite cable&lt;/li&gt;
&lt;li&gt;s.s. voltage along cable&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Sealed end:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;input conductance of the finite cable&lt;/li&gt;
&lt;li&gt;s.s. voltage along cable&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Concatenated Cables:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Compute V(X) along branches, by determining &lt;span class="math"&gt;\(G_{out}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Definitions:&lt;/h3&gt;
&lt;p&gt;Know definitions and related biology of:
- axial resistance, the resistance to current flow along the uniform cable (along the dendrite)
- membrane resistance, the resistance of current flow out of the dendrite
- membrance capacitance, the capacitance of a patch of membrane surface area
- derive cable equation as a model of concatenated RC circuits&lt;/p&gt;
&lt;h3&gt;Compartmental Models:&lt;/h3&gt;
&lt;p&gt;Be able to derive system of equations into a matrix form for a linear time-invariant system &lt;/p&gt;
&lt;p&gt;Understand how transfer resistances work. Understand how distributing synaptic inputs works. &lt;/p&gt;
&lt;p&gt;Understand how coincidence detection (AND operator), shunting inhibition (AND-NOT operator).&lt;/p&gt;
&lt;h2&gt;Study&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Definitions of cable theory and biological relations&lt;/li&gt;
&lt;li&gt;Solving single cable equation and different boundary conditions&lt;/li&gt;
&lt;li&gt;Derivation of cable equation under different boundary conditions&lt;/li&gt;
&lt;li&gt;Deriving LTI system from compartmental models of cables&lt;/li&gt;
&lt;li&gt;Derive transfer resistances&lt;/li&gt;
&lt;li&gt;Derive AND operator (coincidence detection)&lt;/li&gt;
&lt;li&gt;Derive AND-NOT operator (shunting inhibition)&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;2. Generalized Linear Models:&lt;/h1&gt;
&lt;h2&gt;General Form of Exponential Family Distribution&lt;/h2&gt;
&lt;p&gt;Be able to define all the terms, such as: natural paramters, sufficient statistic, natural link function, dispersion parameter&lt;/p&gt;
&lt;h2&gt;Normal Linear Regression&lt;/h2&gt;
&lt;h2&gt;Logistic Regression&lt;/h2&gt;
&lt;h2&gt;Poisson Regression&lt;/h2&gt;
&lt;h2&gt;Solving GLM Methods:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Penalized Quasi-likelihood&lt;/li&gt;
&lt;li&gt;Laplace's Method&lt;/li&gt;
&lt;li&gt;Adaptive Gaussian Quadrature&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Goodness of Fit&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Chi-square test&lt;/li&gt;
&lt;li&gt;Kolmogorov Statistic&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;3. Kalman Filter&lt;/h1&gt;
&lt;h2&gt;Stochastic State-Space Model&lt;/h2&gt;
&lt;p&gt;Here, list the linear, time-invariant system with a state evolution equation and measurement/observation equation.&lt;/p&gt;
&lt;h2&gt;Assumptions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;state and observation noises are independent, zero-mean Gaussian white processes with some defined covariances&lt;/li&gt;
&lt;li&gt;initial state x_0 is a Gaussian R.V. independent of the state/observation noises&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Derivation&lt;/h2&gt;
&lt;p&gt;Derive the Kalman filter equations for the state update, covarariance estimates&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Measurement update&lt;/li&gt;
&lt;li&gt;Time update&lt;/li&gt;
&lt;li&gt;Combined Update&lt;/li&gt;
&lt;li&gt;Covariance Update&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Relation to a Least-Squares Problem&lt;/h2&gt;
&lt;h2&gt;Relation to a Bayesian Maximum Aposteri Estimation&lt;/h2&gt;
&lt;h1&gt;4. Expectation Maximization&lt;/h1&gt;
&lt;h2&gt;Basic Idea&lt;/h2&gt;
&lt;p&gt;By computing the likelihood of your unknown parameters, based on known outcomes. You can perform maximum likelihood estimation to get the best estimate of the unknown parameters&lt;/p&gt;
&lt;h1&gt;5. K-Means Algorithm&lt;/h1&gt;
&lt;h2&gt;Cost Function&lt;/h2&gt;
&lt;p&gt;The cost function is attempting to minimize the distortion (distance to centers) for every point in the set of data points, S.&lt;/p&gt;
&lt;h2&gt;The Algorithm&lt;/h2&gt;
&lt;p&gt;'''
Input: k clusters
initialize centers: z_1, ..., z_k \in \real^d and clusters C_1, ..., C_k
repeat until there is no further change in L(z, C):
    for each j (data point): C_j &amp;lt;- {x \in S whose closest center is z_j}
    for each j (data point): z_j &amp;lt;- mean(C_j) 
'''&lt;/p&gt;
&lt;p&gt;Walk through for a small example to see how the algorithm works:
(0, 0, 0)
(0, 0.5, 1)
(1, 3, 2)
(4, 5, 6)
(2, 3, 1)
(5, 2, 0)
(0, 1, 0)
(1, 1, 0)
(2, 1, 0)&lt;/p&gt;
&lt;h2&gt;Initialization&lt;/h2&gt;
&lt;h2&gt;Convergence&lt;/h2&gt;
&lt;p&gt;Show that the cost monotonically decreases, so the algorithm will converge at least in the sense of cost decreasing to a non-changing amount.&lt;/p&gt;
&lt;h1&gt;6. Sensory Pathways and Systems in Neuroscience&lt;/h1&gt;
&lt;h1&gt;7. Motor Pathways and Systems in Neuroscience&lt;/h1&gt;
&lt;h1&gt;8. General Systems in Neuroscience&lt;/h1&gt;
&lt;h1&gt;9. Gaussian Mixture Models&lt;/h1&gt;
&lt;h1&gt;FAQ&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Q: Where should I stay? 
A: Generally, I'm an advocate of staying downtown because then everything is walking distance. If not, I would try to stay within walking distance to a subway station. Stay away from South Chicago because that is a very crime-ridden area.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Q: I only have a day in Seoul, where should I go?
A: Purple pig, Girl on the goat, Lou Malnati's to eat. Cloud Gate / Millenium Park and the River Walk to see. Signature Room for a night time drink if you have the time.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="doctoral board oral"></category><category term="phd"></category><category term="johns hopkins"></category></entry></feed>