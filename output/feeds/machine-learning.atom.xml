<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Adam Li - Machine Learning</title><link href="/" rel="alternate"></link><link href="/feeds/machine-learning.atom.xml" rel="self"></link><id>/</id><updated>2019-06-18T00:00:00-04:00</updated><entry><title>Linear Gaussian Models</title><link href="/blog/2019/06/gaussian-generative-models/" rel="alternate"></link><published>2019-06-18T00:00:00-04:00</published><updated>2019-06-18T00:00:00-04:00</updated><author><name>Adam Li</name></author><id>tag:None,2019-06-18:/blog/2019/06/gaussian-generative-models/</id><summary type="html">&lt;p&gt;An overview of linear gaussian models and how in general, they fall under the learning procedure (system idenfitication) of Expectation-Maximization.&lt;/p&gt;</summary><content type="html">&lt;!-- MarkdownTOC --&gt;

&lt;ul&gt;
&lt;li&gt;Background&lt;/li&gt;
&lt;li&gt;Methods&lt;ul&gt;
&lt;li&gt;Most General Linear Gaussian Model&lt;/li&gt;
&lt;li&gt;General Expectation Maximization&lt;/li&gt;
&lt;li&gt;Kalman Filter/Smoothing&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Important Models And Connections With Control Theory&lt;ul&gt;
&lt;li&gt;Static Models (i.e. time is not a factor)&lt;ul&gt;
&lt;li&gt;Continuous State&lt;/li&gt;
&lt;li&gt;Discrete State&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dynamical Models (i.e. time is a factor)&lt;ul&gt;
&lt;li&gt;Continuous State - Kalman Filter Models&lt;/li&gt;
&lt;li&gt;Discrete State - Hidden Markov Models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Non-Gaussian Models (i.e. noise terms are no longer normally distributed)&lt;/li&gt;
&lt;li&gt;Control Theory Type Problems&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Conclusions for Gaussian Linear Models
        - Dynamic Version of PCA/KMeans?&lt;/li&gt;
&lt;li&gt;Extensions of Gaussian Linear Models&lt;ul&gt;
&lt;li&gt;Low Rank Tensors (to generalize space, time and other variable dimensions!)&lt;/li&gt;
&lt;li&gt;Deep Kalman Filter&lt;ul&gt;
&lt;li&gt;Caveats of the DKF&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Deep Extend To Control (EC2)&lt;/li&gt;
&lt;li&gt;Deep Variational Bayes Filter&lt;/li&gt;
&lt;li&gt;Deep Koopman Model&lt;ul&gt;
&lt;li&gt;Quick Koopman Theory Overview&lt;/li&gt;
&lt;li&gt;Now the Deep Koopman Model&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Deep Kalman Variational Autoencoders (DKVAE)&lt;/li&gt;
&lt;li&gt;Possibilities for Improved Deep State Space Models (DSSM)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;References:&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;Recently, I re-read this 1999 paper on Linear Gaussian models and I am pretty amazed at how deep this paper is in unifying the different common data analysis methods and linear models under one framework. We think of principal component analysis (PCA), or Gaussian mixture models (GMM), or Kalman filter models (KF) all as disparate ways to model data, but this review is able to bring them under the umbrella of the Expectation Maximization (EM). I wanted to highlight for myself (and anyone reading) the key high level concepts and insights and also extend these to talk about control systems.&lt;/p&gt;
&lt;p&gt;At a very simplifying level, linear gaussian models are heavily used in all branches of engineering from control systems, to data analysis. Expectation maximization is an iterative learning algorithm for learning some optimal parameters in a probabilistic model. In this blog post, I attempt to review the main overarching concepts that I believe are important and then also provide some recent work that draws upon the theory of linear gaussian modeling. Specifically, I look at tensor modeling (i.e. a generalization of a linear time-invariant state space model), deep neural networks to provide end-to-end learning of a state space system and variations of these deep state space models. The goal of this post is to provide the reader with a broad overview of the fundamentals that drive linear modeling to the recent state-of-the-art advancements in deep learning that draw from "older" fundamentals.&lt;/p&gt;
&lt;h1&gt;Methods&lt;/h1&gt;
&lt;h2&gt;Most General Linear Gaussian Model&lt;/h2&gt;
&lt;p&gt;Here is the most general form of the linear latent state-space model.&lt;/p&gt;
&lt;div class="math"&gt;$$\dot{x}(t) = Ax(t) + Bu(t) + w$$&lt;/div&gt;
&lt;div class="math"&gt;$$y(t) = Cx(t) + Du(t) + v$$&lt;/div&gt;
&lt;p&gt;where: &lt;span class="math"&gt;\(w \approx N(0,Q)\)&lt;/span&gt; and &lt;span class="math"&gt;\(v \approx N(0,R)\)&lt;/span&gt; are the state and output noise terms that we assume to be normally distributed (i.e. Gaussian).&lt;/p&gt;
&lt;p&gt;The dimensionality of the terms are:
&lt;em&gt; &lt;span class="math"&gt;\(x, w \in R^{n}\)&lt;/span&gt;
&lt;/em&gt; &lt;span class="math"&gt;\(y, v \in R^{p}\)&lt;/span&gt;
* &lt;span class="math"&gt;\(u \in R^{k}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Some jargon for folks:
&lt;em&gt; x is the state variable, generally considered "hidden", or part of the "latent space" (i.e. some subspace of your data that you don't know)
&lt;/em&gt; y is the observation variable, generally considered measured, or observed (i.e. signals in your data)
&lt;em&gt; u is our control signal, that we input
&lt;/em&gt; A is our state transition matrix governing how states change 
&lt;em&gt; B is our control matrix
&lt;/em&gt; C is our observation matrix
&lt;em&gt; D is our feedthrough matrix
&lt;/em&gt; w and v are our random variable noise terms that we may (sometimes) assume Normally distributed&lt;/p&gt;
&lt;p&gt;Then the A, B, C, D matrices have their respecting dimensionality. In general, we assume y is observed and measured.&lt;/p&gt;
&lt;h2&gt;General Expectation Maximization&lt;/h2&gt;
&lt;p&gt;The general expectation maximization boils down to two distinct steps: 1) Computing the expecation under a certain generative model (i.e. computing the expected states and covariances) and 2) Maximizing the likelihood given the states (i.e. computing the optimal parameters). In the general Gaussian model, these parameters are &lt;span class="math"&gt;\(\theta = \{A, C, B, D, Q, R\}\)&lt;/span&gt;. &lt;/p&gt;
&lt;h2&gt;Kalman Filter/Smoothing&lt;/h2&gt;
&lt;p&gt;The general Kalman filter assumes Gaussian noise, which we have here. Filtering is the problem of predicting the state &lt;span class="math"&gt;\(x(t)\)&lt;/span&gt; given all the observations up to time t. Smoothing is the problem of predicting the state &lt;span class="math"&gt;\(x(t)\)&lt;/span&gt; given all the observations we have (i.e. over entire window of observation of length T).&lt;/p&gt;
&lt;h1&gt;Important Models And Connections With Control Theory&lt;/h1&gt;
&lt;h2&gt;Static Models (i.e. time is not a factor)&lt;/h2&gt;
&lt;p&gt;Here, we are dealing with just data points (i.e. sets of x's, y's), so there is no notion of time dependency. This simplifies the general model, so that A = 0. &lt;/p&gt;
&lt;p&gt;General inference on model:&lt;/p&gt;
&lt;div class="math"&gt;$$P(x | y) = \frac{P(y|x)P(x)}{P(y)} = \frac{N(Cx, R) N(0, I)}{N(0, CC^T + R)}$$&lt;/div&gt;
&lt;div class="math"&gt;$$P(x|y) = N(\beta y, I - \beta C), \ \beta=C^T(CC^T + R)^{-1}$$&lt;/div&gt;
&lt;h3&gt;Continuous State&lt;/h3&gt;
&lt;p&gt;Here, we presume that the state space is continuous (i.e. x is a continuous variable).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;(Sensible) Principal Component Analysis (PCA)
PCA is probably one of the most common dimensionality reduction techniques, which at the end of the day (for you lin alg folks) boils down to Singular Value Decomposition (SVD). Here we assume the following:&lt;/li&gt;
&lt;li&gt;the observation noise R is a multiple of the identity matrix (i.e. &lt;span class="math"&gt;\(R=\alpha I\)&lt;/span&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note that because R is not 0, then we have some noise in the state variables, so this is very similar to probabilistic PCA. &lt;/p&gt;
&lt;p&gt;When &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; goes to 0, then the noise on the states goes to 0, while Q is still finite. This means that the only noise we assume in our model comes from our observations of the data. A naive approach would simply take the observed covariance matrix of our data, apply SVD to obtain the singular vector matrices and the singular values. The columns of C then are the principle components of PCA. The values of our latent space vector x are the principle values (i.e. singular values of our covariance matrix).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Maximum Likelihood Factor Analysis
In factor analysis, we assume the following:&lt;/li&gt;
&lt;li&gt;that the observation covariance matrix, R, is diagonal&lt;/li&gt;
&lt;li&gt;state noise Q is the identity matrix &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;X are considered the factors, and the key assumption assuming R is diagonal means that the model wants to put all the covariance structure in our observed data (i.e. y variables) into the unique coordinates of R. Note that if R is diagonal, then the off-diagonals (i.e. the covariances) are equal to 0. &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Summary
In both static modeling procedures, where we assume our latent space and observations are static, we can solve them using EM by using the general inference equations described above for the static model. P(x|y) gives you the inference estimates of states (i.e. x) for a given set of parameters (in this case: C and R). Then for a given set of states (i.e. x), we can maximize wrt the log-likelihood of our model to recover new estimates of C and R. This is the EM algorithm! &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Quite fascinating that they wrapped this under one umbrella when most people normally look at solving for example PCA using SVD.&lt;/p&gt;
&lt;h3&gt;Discrete State&lt;/h3&gt;
&lt;p&gt;Here, we presume x is a discrete variable.
1. Gaussian Mixture Models
If x is a discrete probability distribution controlled by the distribution of the noise, w. The mean &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and covariance Q parametrize the distribution of x. Now x is modeled by:&lt;/p&gt;
&lt;div class="math"&gt;$$x = WTA(w)$$&lt;/div&gt;
&lt;p&gt;where WTA is the winner take all function applied to the vector w, so x becomes a unit vector of size n. Now the interpretation of x is the mixture weights (i.e. how much each data point y belongs to each cluster). The columns of matrix C represent the cluster means. This is solved via EM also.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Vector Quantization (K-means) Models
When the observation noise approaches 0 (i.e. R approaches 0), then we arrive at the k-means algorithm formulation, which can be solved via EM also. So now, P(x|y) is a single point, since there is no noise in the y term, and it is all governed by the noise in the k clusters defined in the state term. &lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Dynamical Models (i.e. time is a factor)&lt;/h2&gt;
&lt;p&gt;Here, we now deal with the fact that time is a factor in our model. So A is no longer the 0 matrix. &lt;/p&gt;
&lt;h3&gt;Continuous State - Kalman Filter Models&lt;/h3&gt;
&lt;p&gt;The model is generated according to the general model with the noise terms all being independently and identically distributed. This is just solved via the Kalman filter and smoothing algorithms.&lt;/p&gt;
&lt;p&gt;If all parameters are known, can employ a Maximum Likelihood approach to estimate the states.&lt;/p&gt;
&lt;p&gt;If the parameters are unknown, then EM can be used to iterate on the parameters.&lt;/p&gt;
&lt;h3&gt;Discrete State - Hidden Markov Models&lt;/h3&gt;
&lt;p&gt;Now, we consider when the states are discrete, which lead to Hidden Markov Models. The model for the states is:&lt;/p&gt;
&lt;div class="math"&gt;$$x(t+1) = WTA(Ax(t)+w)$$&lt;/div&gt;
&lt;p&gt;Here, if we constrain Q to be the identity matrix, then it will have the same covariances for all Gaussians. With traditional Hidden Markov models, we usually define a state transition matrix (e.g. normalized columns), which defines how probable different state transitions are from an initial state. Solving for the most likely state sequence employs the famous Viterbi algorithm. Solving the filtering and smoothing problems, then employ EM traditionally.&lt;/p&gt;
&lt;h2&gt;Non-Gaussian Models (i.e. noise terms are no longer normally distributed)&lt;/h2&gt;
&lt;p&gt;This results in a famous class of algorithm known as Independent Component Analaysis. This is a generalization of the PCA. Here, we have a nonlinearity applied to the state model:&lt;/p&gt;
&lt;div class="math"&gt;$$x = g(w)$$&lt;/div&gt;
&lt;p&gt;This converts a Gaussian prior (i.e. noise variable w) into a non-Gaussian prior for the state variable x. Now, x is considered our "blind sources" and y is our observed data. In classical ICA, R is assumed to be infinitesimal, and C is square and full-rank. C is considered a "mixing matrix", that is how to mix the sources to "recover" our observed data.&lt;/p&gt;
&lt;h2&gt;Control Theory Type Problems&lt;/h2&gt;
&lt;p&gt;In control theory and linear systems, we are generally interested in the following problems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Given the general model and outputs measured y, how do we estimate a control input u(t) that controls the dynamics of the state? That is, how do we control the eigenvalues of the system? &lt;ul&gt;
&lt;li&gt;This results in things like state-feedback and the notion of controllability.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Given the general model, how can we estimate the values of the states if we know the rest of the parameters (A, B, C, D)?&lt;ul&gt;
&lt;li&gt;This results in observers and the notion of observability.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Given the general model, how can we estimate an optimal u(t) that follows some constraints and minimizes some cost functional on u? (e.g. constraint on the magnitude of u, or the sparseness of u, etc.)&lt;ul&gt;
&lt;li&gt;This is known as optimal control. It can be deterministic, or stochastic. &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that the Kalman filter/smoothing procedure is an optimal observer under Gaussian noise assumptions. The EM algorithm can be used in general in conjunction with Kalman filter/smoothing to estimate the parameters (A, B, C, D).&lt;/p&gt;
&lt;h1&gt;Conclusions for Gaussian Linear Models&lt;/h1&gt;
&lt;p&gt;In this post, I attempt to summarize some of the main points in the Roweis paper that I thought were relevant to someone with knowledge in Linear Algebra, Probability &amp;amp; Statistics and Linear Dynamical Systems. The nice thing is that all these very common algorithms and methods can be framed using Expectation Maximization. This point of view links together control theory, linear dynamical systems and machine learning. It points to how general linear Gaussian models are and how general Expectation Maximization is. &lt;/p&gt;
&lt;h3&gt;Dynamic Version of PCA/KMeans?&lt;/h3&gt;
&lt;p&gt;What happens if we assume a dynamical model, but instead the output noise is zero (i.e. Q=0)? Here, our states are completely determinable if we have our C matrix. For a linear dynamical system, we can perform PCA to obtain our principle components, which can comprise of our C matrix (i.e. how to go from principle values to observed space). For Hidden Markov Models, we can instead perform vector quantization (i.e. KMeans) to obtain our columns of C. Now, we have to actually estimate the A matrix, which is a first-order Markov dynamic matrix (i.e. governing how states change from time t to t+1). Here it boils down to a simple autoregressive (AR(1)) model in continuous time, or first order Markov chain in the discrete space.&lt;/p&gt;
&lt;h1&gt;Extensions of Gaussian Linear Models&lt;/h1&gt;
&lt;p&gt;Here, I talk about some extensions to Gaussian linear models and relate them to our linear models through the lens of probability and statistics; specifically: variational inference and markov chain monte carlo. These are the main techniques in the estimation of an intractable posterior distribution. Here, we'll assume you have basic Bayesian working knowledge and comfortable with the statistics involved.&lt;/p&gt;
&lt;p&gt;The setup of the problem is similar to that of Linear Gaussian Models.&lt;/p&gt;
&lt;div class="math"&gt;$$\dot{x}(t) = f(x(t)) + g(u(t)) + w$$&lt;/div&gt;
&lt;div class="math"&gt;$$y(t) = h(x(t)) + v$$&lt;/div&gt;
&lt;p&gt;where: &lt;span class="math"&gt;\(w \approx Q(\theta)\)&lt;/span&gt; and &lt;span class="math"&gt;\(v \approx R(\gamma)\)&lt;/span&gt; are the state and output noise terms that we assume to be distributed with some distribution &lt;span class="math"&gt;\(Q,R\)&lt;/span&gt; parametrized by &lt;span class="math"&gt;\(\theta, \gamma\)&lt;/span&gt;. In addition, now &lt;span class="math"&gt;\(f, g, h\)&lt;/span&gt; are all potentially nonlinear functions analogs of A, B, C. If we define some priors on the distribution of noise for the latent variables, we can perform Bayesian inference given our observed signals, y. That is, we are interested in estimating:&lt;/p&gt;
&lt;div class="math"&gt;$$P(x|y) \approx P(y|x) P(x)$$&lt;/div&gt;
&lt;p&gt;Note, that in convention with literature, P(y|x) is our likelihood of the model and P(x) is our assumed prior distribution on our latent state variable. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Variational inference (VI) proceeds by: * 
Fitting the parameters of a family of tractable distributions (e.g. independent Gaussians) to approximate the posterior. For more details, see my blog post on a summary of VI and MCMC.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Markov Chain Monte Carlo proceeds by: * 
Creating a Markov chain that has convergent properties to the true posterior. Samples are drawn from the "proposal" distribution and are either kept, or rejected based on various algorithms (e.g. important sampling, Gibbs sampling, Hamiltonian Monte Carlo, etc.). For more details, see my blog post on a summary of VI and MCMC.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Low Rank Tensors (to generalize space, time and other variable dimensions!)&lt;/h2&gt;
&lt;p&gt;In a Linear Gaussian state space model, in general you are assuming that x, y, u are vectors, while A, B, C are matrices of their respective dimensions. Tensors are higher order generalizations of matrices (e.g. matrices are 2D arrays, while tensors are N-D arrays). So in the context of neural recording data where you have a suite of electrode signals that vary over time, one can think of the system as a 3D tensor (channels X channels X time); this is opposed to a normal linear time-invariant system (i.e. constant A matrix), which would just be represented as a 2D tensor (channels X channels) because time is invariant. Now, just viewing this system as a 3D tensor does not really give much room for improvement because now you are just dealing with a linear time-varying system with N^2 T parameters (N=#channels, T=#time). &lt;/p&gt;
&lt;p&gt;However, in the TVART paper from Rajesh Rao's group, they assume that the system's state transition matrix, A, lies in a low rank tensor. Specifically, this gives the system only a few "unique" A matrices (i.e. system state transition matrices) to choose from at any particular window of data. The number of these unique A matrices is determined by the assumption of the "low-rank" of this tensor. &lt;/p&gt;
&lt;p&gt;One of the shortcomings of this method is that there is no easy way to determine the "low-rankness" of a dataset prior to analyzing it. I imagine that the way to proceed is to use some heuristic measure, such as Bayesian Information Criterion / Aikake's Information Criterion. &lt;/p&gt;
&lt;p&gt;Assuming you have a good choice of the rank, then you can apply some traditional regularizations, such as l1 and l2 regularizations to enforce sparsity and smoothness in the different dimensions of the A 3D tensor (i.e. either in channel space, or in time space). Then the optimal tensor is solved for via convex optimization. The paper uses a combination of conjugate gradient and proximal gradient with Nesterov acceleration for the smooth, and non-smooth objective functions respectively. A review of these optimization terms hopefully can be covered in another blog post on optimization: convex, nonlinear and constrained.&lt;/p&gt;
&lt;h2&gt;Deep Kalman Filter&lt;/h2&gt;
&lt;p&gt;In the paper on the deep Kalman Filter, the authors derive a deep neural network that is a Kalman filter analog. They explicitly assume that the latent state is modeled by a Normal distribution (i.e. &lt;span class="math"&gt;\(x ~ N(\mu, \Sigma)\)&lt;/span&gt;) with possible nonlinear interactions between time steps, and that the observations are distributed according to some family of distributions (e.g. Bernoulli if observations are binary).&lt;/p&gt;
&lt;p&gt;The neural network architectures that seem to work are the: i) q-RNN (filtering) and ii) q-BRNN (smoothing), which are just recurrent and bi-directional recurrent neural network architectures. They perform variational learning of the latent state parameters by maximizing a derived evidence lower bound (ELBO). In the Variational AutoEncoder paper, they talk about how to perform stochastic gradient descent by performing the re-parameterization trick on the Normal distribution.&lt;/p&gt;
&lt;p&gt;They introduce the notion of performing counterfactual inference by introducing an extra variable that the network conditions on. For example, you can include a vectorized variable that determines whether or not you took a drug at time t, or whether or not a specific action was taken. Then during testing, you can modify the conditional variable action and perform forward prediction to determine the x(t+1), ..., x(t+T) steps which predicts what "would have" happened if you took a certain action.&lt;/p&gt;
&lt;h3&gt;Caveats of the DKF&lt;/h3&gt;
&lt;p&gt;In experiments, they show that the DKF does not extract information about time-derivatives very well, which in general lead to problems with forward prediction.&lt;/p&gt;
&lt;h2&gt;Deep Extend To Control (EC2)&lt;/h2&gt;
&lt;p&gt;In this paper, they combine optimal control theory and deep learning.&lt;/p&gt;
&lt;h2&gt;Deep Variational Bayes Filter&lt;/h2&gt;
&lt;p&gt;In this paper, they argue that a suite of previous methods that use a VAE-style for time series is optimized for reconstruction mainly (i.e. not prediction). This will just compress the data in the best possible manner, while allowing the data to still be reconstructed. However, there is no explicit optimization for prediction. In addition, previous methods may not explicitly include the necessary mini-batch data for the model to infer time-derivatives, thus weakening prediction as well.&lt;/p&gt;
&lt;p&gt;First, we want to reparametrize the transition model:&lt;/p&gt;
&lt;div class="math"&gt;$$x(t+1) = f(x(t), u(t), \beta(t))$$&lt;/div&gt;
&lt;h2&gt;Deep Koopman Model&lt;/h2&gt;
&lt;h3&gt;Quick Koopman Theory Overview&lt;/h3&gt;
&lt;p&gt;For the reader (and myself), I assume we have no idea what the Koopman operator is. So first, I will overview some of the basic concepts here, so that I can introduce the Deep Koopman model. See https://www.mit.edu/~arbabi/research/KoopmanIntro.pdf for a great in-depth overview. One should be relatively comfortable with the theory of linear dynamical systems and matrix analysis (specifically the idea of state transition matrices, linearity and eigenvalues/eigenvectors). Recall a linear dynamical system without input:&lt;/p&gt;
&lt;div class="math"&gt;$$\dot{x}(t) = Ax(t)$$&lt;/div&gt;
&lt;p&gt;where A is our linear operator of the form &lt;span class="math"&gt;\(T: \mathbf{R}^n -&amp;gt; \mathbf{R}^n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The idea of the Koopman operator is that any data we collect in a real-world scenario are "observables" of our system that are a function of the underlying state (g). So the Koopman operator is a linear transformation (U) that acts on the state transition operator (A). An eigenvalue decomposition of the Koopman operator yields (possibly infinite-dimensional) eigenfunctions that span the observable space, so now all observables of a dynamical system are linear combinations of the eigenfunctions of the Koopman operator. This is quite fascinating because the idea is that you can break down nonlinear system phenomena in terms of linear Koopman eigenfunctions (nonlinear approximation by linear functions)! &lt;/p&gt;
&lt;div class="math"&gt;$$Ug(x) = g o T(x)$$&lt;/div&gt;
&lt;p&gt;For example, we can look at:
1. Limit Cycles:
This is a nonlinear system property because no linear dynamical system (i.e. x(t+1) = Ax(t)) can generate limit cycles. A limit cycle is parametrized by a period, T. So one can perform Fourier decomposition:&lt;/p&gt;
&lt;div class="math"&gt;$$g(x(t)) = \sum_{k=0}^{\infty} a_j e^{ikt2\pi / T}$$&lt;/div&gt;
&lt;p&gt;The &lt;span class="math"&gt;\(a_j\)&lt;/span&gt; are Fourier coefficients and the exponential term can be constructed as the eigenfunctions of the Koopman operator with eigenvalues &lt;span class="math"&gt;\(\lambda_k = ik2\pi / T\)&lt;/span&gt; (interestingly when eigenvalues are of this form, you have degenerate discrete time-sampling in the sense that you can lose controllability, or observability of dynamical system). &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Hyperbolic Fixed Points&lt;/li&gt;
&lt;li&gt;Basins of Attraction (stable limit cycles)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now in general, you can perform Koopman mode decomposition on a vector of observables (no one just observes one data point). You get:&lt;/p&gt;
&lt;div class="math"&gt;$$U^t g(x) = \sum_{k=0}^{\infty} g_k e^{\lambda_k t} \phi_k(x)$$&lt;/div&gt;
&lt;p&gt;where U is our Koopman operator, g is our observable function of our state, x is our state vector, &lt;span class="math"&gt;\(g_k\)&lt;/span&gt; are the Koopman modes of the observable g at the eigenvalue &lt;span class="math"&gt;\(\lambda_k\)&lt;/span&gt;, &lt;span class="math"&gt;\(\phi_k\)&lt;/span&gt; are the eigenfunctions at their respective eigenvalue.&lt;/p&gt;
&lt;h3&gt;Now the Deep Koopman Model&lt;/h3&gt;
&lt;p&gt;In this paper, they parametrize the observable function using a deep neural network, and then explicitly model the decoding as a Koopman operator to learn the mappings of a latent state back into the observable data. In addition, they add special training scheme of penalizing the loss over consecutive predictive steps, so that the model learns a stable mapping that attempts to keep low error rates over many consecutive predictions.&lt;/p&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2&gt;Deep Kalman Variational Autoencoders (DKVAE)&lt;/h2&gt;
&lt;p&gt;In this paper, the authors formulate yet another variation of the VAE framework that is somewhat similar to the DVBF model, but with a few differences and improvements. Recall that the model for the DVBF was for locally linear transitions:&lt;/p&gt;
&lt;div class="math"&gt;$$x(t+1) = A(t)x(t) + B(t)u(t) + C(t)w(t)$$&lt;/div&gt;
&lt;div class="math"&gt;$$y(t) = H(t)x(t) + y(t)$$&lt;/div&gt;
&lt;p&gt;Here, the variational parameters that parametrize the variational family is &lt;span class="math"&gt;\(v(t) = \{A(t)^{i}, B(t)^{i}, C(t)^{i}\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the DKVAE, the model adds an additional pseudo-latent state, which is of low-dimension between the high dimensional observation space and the linear state space model. &lt;/p&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2&gt;Possibilities for Improved Deep State Space Models (DSSM)&lt;/h2&gt;
&lt;p&gt;It seems that a reoccuring theme right now in deep learning is the integration of older techniques (i.e. Linear State Space Models LSSM) with deep neural networks. Since deep neural networks can act as universal function approximators, that are also able to be trained end-to-end using stochastic gradient descent on data, then many challenges in traditional LSSM that had intractable setups can potentially be overcome. Granted, you will need enough data for each situation you are thinking of, but in theory one can really extend the power of DSSM to provide improved interpretability of a deep learning model.&lt;/p&gt;
&lt;h1&gt;References:&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Roweis S. et al. "A Unifying Review of Linear Gaussian Models".http://mlg.eng.cam.ac.uk/zoubin/papers/lds.pdf&lt;/li&gt;
&lt;li&gt;"Deep Kalman Filter."  https://arxiv.org/abs/1511.05121&lt;/li&gt;
&lt;li&gt;"Deep Variational Bayes Filter." https://arxiv.org/abs/1605.06432&lt;/li&gt;
&lt;li&gt;"Deep Koopman Model." https://arxiv.org/pdf/1805.07472.pdf&lt;/li&gt;
&lt;li&gt;"Kalman VAE." https://arxiv.org/pdf/1710.05741.pdf&lt;/li&gt;
&lt;li&gt;"Time Varying Autoregression with Low-Rank Tensors (TVART)."&lt;/li&gt;
&lt;li&gt;"Introduction to Koopman Theory." https://www.mit.edu/~arbabi/research/KoopmanIntro.pdf&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="phd"></category><category term="machine learning"></category></entry><entry><title>Optimization: Convex, Nonlinear, Unconstrained and Constrained</title><link href="/blog/2019/06/optimization-landscape-overview/" rel="alternate"></link><published>2019-06-18T00:00:00-04:00</published><updated>2019-06-18T00:00:00-04:00</updated><author><name>Adam Li</name></author><id>tag:None,2019-06-18:/blog/2019/06/optimization-landscape-overview/</id><summary type="html">&lt;p&gt;An overview of optimization frameworks and algorithms under different general settings.&lt;/p&gt;</summary><content type="html">&lt;!-- MarkdownTOC --&gt;

&lt;ul&gt;
&lt;li&gt;Background&lt;/li&gt;
&lt;li&gt;Methods&lt;ul&gt;
&lt;li&gt;Convex&lt;ul&gt;
&lt;li&gt;Non-smooth&lt;/li&gt;
&lt;li&gt;Smooth&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Unconstrained&lt;/li&gt;
&lt;li&gt;Constrained&lt;/li&gt;
&lt;li&gt;Nonlinear&lt;ul&gt;
&lt;li&gt;Unconstrained&lt;/li&gt;
&lt;li&gt;Constrained&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Zero-Finding: Newton's Method and the Secant Method&lt;ul&gt;
&lt;li&gt;Newton Directions, General Newton Method and Quasi-Newton Methods&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Programming (Linear, Quadratic and Semidefinite)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stochastic Optimization&lt;/li&gt;
&lt;li&gt;Conclusions&lt;/li&gt;
&lt;li&gt;Current Research and Interesting Papers&lt;ul&gt;
&lt;li&gt;Accelerated Adaptive Moments (ADAM)&lt;/li&gt;
&lt;li&gt;RMS-Prop&lt;/li&gt;
&lt;li&gt;Structured Regularizations and Different Forms&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;References:&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;Recently, I re-read my notes on convex optimization, nonlinear unconstrained optimization and nonlinear constrained optimization. It's quite fascinating how mathematics can break down very general assumptions into different classes of algorithms with certain convergence, convergence rate and computational cost guarantees. In general all these algorithms are iterative algorithms, that solve a certain optimization problem by taking iterations from a starting vector &lt;span class="math"&gt;\(x_0\)&lt;/span&gt;. Obviously, there are other classes of algorithms that can say be solved analytically (i.e. one shot), but we're interested in how to take iterations that converge, converge fast and are computationally efficient. The different classes of algorithms can at a high level be broken down into:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;convex vs nonconvex&lt;/li&gt;
&lt;li&gt;constrained vs unconstrained&lt;/li&gt;
&lt;li&gt;linear vs nonlinear&lt;/li&gt;
&lt;li&gt;differentiable vs non-differentiable (or smooth vs nonsmooth)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In each of these classes of algorithms there are relatively important concepts that are present in all of them such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;regularization (i.e. the addition of a l1, l2, or p-norm operator on some variable of interest) to prevent overfitting, include prior knowledge into the model, and improve tractability&lt;/li&gt;
&lt;li&gt;duality (i.e. the idea of solving a related "dual" problem that has nice properties)&lt;/li&gt;
&lt;li&gt;initialization (i.e. &lt;span class="math"&gt;\(x_0\)&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;step length (i.e. &lt;span class="math"&gt;\(\alpha_t\)&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;direction of algorithm iteration (i.e. &lt;span class="math"&gt;\(g_t\)&lt;/span&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In general, one is interested in necessary and/or sufficient conditions for optimality and the following questions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;does an algorithm converge and what are the necessary assumptions to do so?&lt;/li&gt;
&lt;li&gt;what is the convergence rate of an algorithm (i.e. how many iterations to reach a bound on the error rate)?&lt;/li&gt;
&lt;li&gt;what is the computational cost per iteration of the algorithm (i.e. how many function evaluations, jacobian evaluations, or hessian evaluations does one need)?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;What I will not overview is combinatorial and stochastic optimization. I will plan on adding the general concepts once I've reviewed linear, quadratic, and semidefinite programming, as well as stochastic gradient descent. &lt;/p&gt;
&lt;h1&gt;Methods&lt;/h1&gt;
&lt;p&gt;Here, I preface the landscape of optimization algorithms with the general objective function:&lt;/p&gt;
&lt;div class="math"&gt;$$minimize_{x \in X} f(x) \ s.t. \ g(x)=0,\ h(x) \le 0$$&lt;/div&gt;
&lt;div class="math"&gt;$$X \subset \mathbf{R}^n$$&lt;/div&gt;
&lt;div class="math"&gt;$$f: X -&amp;gt; \mathbf{R}$$&lt;/div&gt;
&lt;div class="math"&gt;$$g: X -&amp;gt; \mathbf{R}^k$$&lt;/div&gt;
&lt;div class="math"&gt;$$h: X -&amp;gt; \mathbf{R}^l$$&lt;/div&gt;
&lt;p&gt;There are k equality constraints, and l inequality constraints. X is our feasible set, f is our evaluation function (think loss/cost function), g is our equality constraint function, and h is our inequality constraint function. x is our variable of interest that we want in the end that satisfies this minimization problem.&lt;/p&gt;
&lt;p&gt;A very general iteration looks something like this:&lt;/p&gt;
&lt;div class="math"&gt;$$x_{k+1} = x_k - \alpha_k g_k (x_k)$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is the step size, g is the step direction. Note that &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; can either be a vector, or scalar (depending on if you want to step uniformly, or with varying magnitudes in the direction vector) and g is a in general a vector that denotes the directionality in the space of x (i.e. &lt;span class="math"&gt;\(R^n\)&lt;/span&gt;).&lt;/p&gt;
&lt;h2&gt;Convex&lt;/h2&gt;
&lt;p&gt;In this section, we make the assumption that f is convex, and in general the constraint functions are convex. Assuming convexity provides a couple of strong guarantees:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the minimum we find is a global minimum, so we don't have to say rerun the algorithm with multiple initializations&lt;/li&gt;
&lt;li&gt;in general strong duality applies, so there is a zero duality gap (I think)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is a whole theory of convex analysis and convex optimization, but here we review the main algorithms that come out of this convex assumption. In general, the algorithms can be broken down into non-smooth vs smooth (where f is either differentiable, or not). If f is not differentiable, a technical detail is that we still assume f is Lipschitz-continuous. &lt;/p&gt;
&lt;h3&gt;Non-smooth&lt;/h3&gt;
&lt;p&gt;Here, we are dealing with convex functions that are not differentiable. We are able to circumvent the issue with the use of subgradients and proximal operators. This leads to algorithms like:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Subgradient Descent Algorithm
This is essentially gradient descent, but using the subgradient. It's a basic algorithm that has guaranteed convergence with some assumptions on the choices of your step sizes, &lt;span class="math"&gt;\(\alpha_k\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Proximal Gradient Descent Algorithm
This defines a proximal operator that IS differentiable, so that we can take gradients of the proximal operator to define our direction, and then take corresponding steps. This leads to algorithms like the iterative shrinkage threshold operator (ISTA).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;Acceleration (Nesterov)&lt;/em&gt;
Armed with the proximal gradient, one can apply acceleration techniques of the form:&lt;/p&gt;
&lt;div class="math"&gt;$$ $$&lt;/div&gt;
&lt;p&gt;, which leads to algorithms like FISTA (fast ISTA).&lt;/p&gt;
&lt;h3&gt;Smooth&lt;/h3&gt;
&lt;p&gt;Here, we are dealing with convex functions that ARE differentiable (i.e you can take the derivative). Taking the derivative gives you powerful first-order information. This leads to algorithms like:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Gradient Descent&lt;/li&gt;
&lt;li&gt;Conjugate Gradient Descent&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Unconstrained&lt;/h2&gt;
&lt;p&gt;In this section, we comment on the idea of unconstrained optimization. That is the objective function does not have g, or h terms (equality, or inequality constraints). In general, this leads to two classes of algorithms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;line-search methods (e.g. Armijo line-search and Wolfe conditional line search)&lt;/li&gt;
&lt;li&gt;trust-region methods&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For convex and linear optimization problems, generally you don't need such methods, so we restrict overview until we reach the section on Nonlinear optimization.&lt;/p&gt;
&lt;h2&gt;Constrained&lt;/h2&gt;
&lt;p&gt;In this section, we allow for constraints either in the form of equality, and/or inequality constraints. The idea of adding constraints is really fascinating because in most (almost all) real world problems, you can formulate a loss function to optimize (based on some metric) that generally has constraints built in! This is due to the nature of the problem. For example, if you want to optimize usage of fuel in a car, you are constrained by the amount of fuel you can even have and the fact that fuel can never be negative! In general, adding constraints helps the optimization problem achieve better solutions. In order to analyze a constrained optimization problem, the strategy is to perform a "conversion" into an unconstrained problem. This leads to the definition of a Lagrangian function (draws upon physics):&lt;/p&gt;
&lt;div class="math"&gt;$$L(x,y,\lambda,\mu) = $$&lt;/div&gt;
&lt;p&gt;There is a wealth of theory behind constraint optimization with some of the basics drawing from the idea of Lagrange multipliers (that handle equality constraints). One can generalize this notion (it's quite beautiful actually) to inequality constraints, which then handle all possible constraints you might have. This generalization leads to the notion of the Karusch-Kuhn-Tucker (KKT) conditions for optimality. The KKT conditions are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;stationarity:&lt;/li&gt;
&lt;li&gt;primal feasibility:&lt;/li&gt;
&lt;li&gt;dual feasibility:&lt;/li&gt;
&lt;li&gt;complementary slackness: &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In general, these define necessary conditions for optimality, and in some special cases, they can define also sufficient conditions. Basically, with KKT conditions, you can convert any constrained optimization problem into an unconstrained version with the Lagrangian.&lt;/p&gt;
&lt;p&gt;I don't actually talk about the algorithms here because they get quite complex, but I will cover them in the "Programming (Linear, Quadratic and Semidefinite)" section in the future.&lt;/p&gt;
&lt;h2&gt;Nonlinear&lt;/h2&gt;
&lt;p&gt;In this section, we now deal with the possibility of nonlinear functions f, and possibly nonlinear constraint functions. This then leads to the problem that in general, we can not find a global minima of the optimization problem. We simply find local minimums that may, or may not be a useful solution and possibly rerun the algorithms with multiple intializations. &lt;/p&gt;
&lt;h3&gt;Unconstrained&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Line-Search Methods
The purpose of line search methods is to define a direction of search first and then perform a line search to determine a good step size. The procedure is as follows:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;solve for a descent direction &lt;span class="math"&gt;\(B_k\)&lt;/span&gt;. In general, it is difficult to get this for high-dimensional systems exactly because you can't compute the Hessian.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;perform a line search to determine the step size &lt;span class="math"&gt;\(\alpha_k\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here, we review two main line search methods: 
&lt;em&gt; exact line search: Armijo line search (backtracking line search) and 
&lt;/em&gt; inexact line search with satisfied Wolfe conditions.&lt;/p&gt;
&lt;p&gt;The wolfe conditions are conditions for choosing a step length, &lt;span class="math"&gt;\(\alpha_k\)&lt;/span&gt;, given a descent direction &lt;span class="math"&gt;\(p_k\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(Armijo rule) &lt;span class="math"&gt;\(f(x_k + \alpha_k p_k) \le f(x_k) + c_1 \alpha_k p_k^T \Nabla f(x_k)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;(curvature condition) -p_k^T \Nabla f(x_k + \alpha_k p_k) \le -c_2 p_k^T \Nabla f(x_k)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Armijo rule ensures that the step length decreases f sufficiently. The curvature condition ensures that the slope has been reduced sufficiently. With the strong Wolfe conditions, there is a different curvature condition:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(|p_k^T \Nabla f(x_k + \alpha_k p_k)| \le c_2 |p_k^T \Nabla f(x_k)|\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;with &lt;span class="math"&gt;\(0 &amp;lt; c_1 &amp;lt; c_2 &amp;lt; 1\)&lt;/span&gt;. If &lt;span class="math"&gt;\(p_k\)&lt;/span&gt; is a descent direction, then we just need to satisfy:&lt;/p&gt;
&lt;div class="math"&gt;$$p_k^T \Nabla f(x_k) &amp;lt; 0$$&lt;/div&gt;
&lt;p&gt;with &lt;span class="math"&gt;\(p_k = -\Nabla f(x_k)\)&lt;/span&gt; as the gradient direction, or with &lt;span class="math"&gt;\(p_k = - H^{-1} \Nabla f(x_k)\)&lt;/span&gt; with H being positive definite as the Newton-Raphson direction. &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Trust Region Methods
The purpose of trust region methods is to define a region that is "trustworthy" to move in the direction of a local optima. The idea is to first perform a "line search" and then determine the direction of search. A trust region algorithm solves a subproblem that uses the gradient information and Hessian (or approximation) information.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Constrained&lt;/h3&gt;
&lt;p&gt;See section on Programming. &lt;/p&gt;
&lt;h2&gt;Zero-Finding: Newton's Method and the Secant Method&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Secant method
This method essentaially uses an iteration of secant lines and their roots to better approximate a root of the original function of interest, f. It is similar to a finite-difference approximation of Newton's method.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="math"&gt;$$x_{k+1} = x_k - f(x_k) \frac{x_k - x_{k-1}}{f(x_k) - f(x_{k-1})}$$&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;Newton's method&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="math"&gt;$$x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)}$$&lt;/div&gt;
&lt;p&gt;This Newton's method can be approximated when Jacobians and Hessians are too expensive, by using the class of quasi-newton methods, such as the BFGS method.&lt;/p&gt;
&lt;h3&gt;Newton Directions, General Newton Method and Quasi-Newton Methods&lt;/h3&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2&gt;Programming (Linear, Quadratic and Semidefinite)&lt;/h2&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h1&gt;Stochastic Optimization&lt;/h1&gt;
&lt;p&gt;This section deserves a header of its own because of its widespread usage nowadays in training deep neural networks and actually any optimization problem that can't necesesarily fit into RAM. 
TBD&lt;/p&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;h1&gt;Current Research and Interesting Papers&lt;/h1&gt;
&lt;h2&gt;Accelerated Adaptive Moments (ADAM)&lt;/h2&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2&gt;RMS-Prop&lt;/h2&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2&gt;Structured Regularizations and Different Forms&lt;/h2&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h1&gt;References:&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Cvx Course, JHU. https://sites.google.com/site/danielprobinson/convex-optimization&lt;/li&gt;
&lt;li&gt;"Convex Optimization." https://web.stanford.edu/~boyd/cvxbook/&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="phd"></category><category term="machine learning"></category></entry></feed>