<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Adam Li - Machine Learning</title><link href="/" rel="alternate"></link><link href="/feeds/machine-learning.atom.xml" rel="self"></link><id>/</id><updated>2019-06-18T00:00:00-04:00</updated><entry><title>Linear Gaussian Models</title><link href="/blog/2019/06/gaussian-generative-models/" rel="alternate"></link><published>2019-06-18T00:00:00-04:00</published><updated>2019-06-18T00:00:00-04:00</updated><author><name>Adam Li</name></author><id>tag:None,2019-06-18:/blog/2019/06/gaussian-generative-models/</id><summary type="html">&lt;p&gt;An overview of linear gaussian models and how in general, they fall under the learning procedure (system idenfitication) of Expectation-Maximization.&lt;/p&gt;</summary><content type="html">&lt;!-- MarkdownTOC --&gt;

&lt;ul&gt;
&lt;li&gt;Background&lt;/li&gt;
&lt;li&gt;Methods&lt;/li&gt;
&lt;li&gt;Conclusions&lt;/li&gt;
&lt;li&gt;References:&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;Recently, I re-read this 1999 paper on Linear Gaussian models and I am pretty amazed at how deep this paper is in unifying the different common data analysis methods and linear models under one framework. We think of principal component analysis (PCA), or Gaussian mixture models (GMM), or Kalman filter models (KF) all as disparate ways to model data, but this review is able to bring them under the umbrella of the Expectation Maximization (EM). &lt;/p&gt;
&lt;h1&gt;Methods&lt;/h1&gt;
&lt;p&gt;.. math::&lt;/p&gt;
&lt;p&gt;\alpha = 10^2&lt;/p&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;h1&gt;References:&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Roweis S. et al. "A Unifying Review of Linear Gaussian Models".http://mlg.eng.cam.ac.uk/zoubin/papers/lds.pdf&lt;/li&gt;
&lt;/ol&gt;</content><category term="phd"></category><category term="machine learning"></category></entry></feed>