<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# fb: https://www.facebook.com/2008/fbml">
<head>
    <title>Linear Gaussian Models - Adam Li</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">



<link rel="canonical" href="/blog/2019/06/gaussian-generative-models/">

        <meta name="author" content="Adam Li" />
        <meta name="keywords" content="phd,machine learning" />
        <meta name="description" content="An overview of linear gaussian models and how in general, they fall under the learning procedure (system idenfitication) of Expectation-Maximization." />

        <meta property="og:site_name" content="Adam Li" />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="Linear Gaussian Models"/>
        <meta property="og:url" content="/blog/2019/06/gaussian-generative-models/"/>
        <meta property="og:description" content="An overview of linear gaussian models and how in general, they fall under the learning procedure (system idenfitication) of Expectation-Maximization."/>
        <meta property="article:published_time" content="2019-06-18" />
            <meta property="article:section" content="Machine Learning" />
            <meta property="article:tag" content="phd" />
            <meta property="article:tag" content="machine learning" />
            <meta property="article:author" content="Adam Li" />



    <!-- Bootstrap -->
        <link rel="stylesheet" href="/theme/css/bootstrap.min.css" type="text/css"/>
    <link href="/theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="/theme/css/pygments/native.css" rel="stylesheet">
    <link rel="stylesheet" href="/theme/css/style.css" type="text/css"/>

        <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Adam Li ATOM Feed"/>



        <link href="/feeds/machine-learning.atom.xml" type="application/atom+xml" rel="alternate"
              title="Adam Li Machine Learning ATOM Feed"/>

</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
	<div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="/" class="navbar-brand">
Adam Li            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                    <li><a href="/categories.html">Blog</a></li>
                    <li><a href="/archives.html">Timeline</a></li>
                    <li><a href="/tags.html">Tags</a></li>
                    <li><a href="/pdfs/AdamLi_CV.pdf">Curriculum Vitae</a></li>
                         <li><a href="/contact/">
                             Contact
                          </a></li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->
<!-- Banner -->
<!-- End Banner -->
<div class="container">
    <div class="row">
        <div class="col-sm-9">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="/blog/2019/06/gaussian-generative-models/"
                       rel="bookmark"
                       title="Permalink to Linear Gaussian Models">
                        Linear Gaussian Models
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2019-06-18T00:00:00-04:00"> Tue 18 June 2019</time>
    </span>





<span class="label label-default">Tags</span>
	<a href="/tag/phd.html">phd</a>
        /
	<a href="/tag/machine-learning.html">machine learning</a>
    
</footer><!-- /.post-info -->                    </div>
                </div>
                <!-- MarkdownTOC -->

<ul>
<li>Background</li>
<li>Methods<ul>
<li>Most General Linear Gaussian Model</li>
<li>General Expectation Maximization</li>
<li>Kalman Filter/Smoothing</li>
</ul>
</li>
<li>Important Models And Connections With Control Theory<ul>
<li>Static Models (i.e. time is not a factor)<ul>
<li>Continuous State</li>
<li>Discrete State</li>
</ul>
</li>
<li>Dynamical Models (i.e. time is a factor)<ul>
<li>Continuous State - Kalman Filter Models</li>
<li>Discrete State - Hidden Markov Models</li>
</ul>
</li>
<li>Non-Gaussian Models (i.e. noise terms are no longer normally distributed)</li>
<li>Control Theory Type Problems</li>
</ul>
</li>
<li>Conclusions
        - Dynamic Version of PCA/KMeans?</li>
<li>References:</li>
</ul>
<!-- /MarkdownTOC -->

<h1>Background</h1>
<p>Recently, I re-read this 1999 paper on Linear Gaussian models and I am pretty amazed at how deep this paper is in unifying the different common data analysis methods and linear models under one framework. We think of principal component analysis (PCA), or Gaussian mixture models (GMM), or Kalman filter models (KF) all as disparate ways to model data, but this review is able to bring them under the umbrella of the Expectation Maximization (EM). I wanted to highlight for myself (and anyone reading) the key high level concepts and insights and also extend these to talk about control systems.</p>
<p>At a very simplifying level, linear gaussian models are heavily used in all branches of engineering from control systems, to data analysis. Expectation maximization is an iterative learning algorithm for learning some optimal parameters in a probabilistic model.</p>
<h1>Methods</h1>
<h2>Most General Linear Gaussian Model</h2>
<p>Here is the most general form of the linear latent state-space model.</p>
<div class="math">$$\dot{x}(t) = Ax(t) + Bu(t) + w$$</div>
<div class="math">$$y(t) = Cx(t) + Du(t) + v$$</div>
<p>where: <span class="math">\(w \approx N(0,Q)\)</span> and <span class="math">\(v \approx N(0,R)\)</span> are the state and output noise terms that we assume to be normally distributed (i.e. Gaussian).</p>
<p>The dimensionality of the terms are:
<em> <span class="math">\(x, w \in R^{n}\)</span>
</em> <span class="math">\(y, v \in R^{p}\)</span>
* <span class="math">\(u \in R^{k}\)</span></p>
<p>Some jargon for folks:
<em> x is the state variable, generally considered "hidden", or part of the "latent space" (i.e. some subspace of your data that you don't know)
</em> y is the observation variable, generally considered measured, or observed (i.e. signals in your data)
<em> u is our control signal, that we input
</em> A is our state transition matrix governing how states change 
<em> B is our control matrix
</em> C is our observation matrix
<em> D is our feedthrough matrix
</em> w and v are our random variable noise terms that we may (sometimes) assume Normally distributed</p>
<p>Then the A, B, C, D matrices have their respecting dimensionality. In general, we assume y is observed and measured.</p>
<h2>General Expectation Maximization</h2>
<p>The general expectation maximization boils down to two distinct steps: 1) Computing the expecation under a certain generative model (i.e. computing the expected states and covariances) and 2) Maximizing the likelihood given the states (i.e. computing the optimal parameters). In the general Gaussian model, these parameters are <span class="math">\(\theta = \{A, C, B, D, Q, R\}\)</span>. </p>
<h2>Kalman Filter/Smoothing</h2>
<p>The general Kalman filter assumes Gaussian noise, which we have here. Filtering is the problem of predicting the state <span class="math">\(x(t)\)</span> given all the observations up to time t. Smoothing is the problem of predicting the state <span class="math">\(x(t)\)</span> given all the observations we have (i.e. over entire window of observation of length T).</p>
<h1>Important Models And Connections With Control Theory</h1>
<h2>Static Models (i.e. time is not a factor)</h2>
<p>Here, we are dealing with just data points (i.e. sets of x's, y's), so there is no notion of time dependency. This simplifies the general model, so that A = 0. </p>
<p>General inference on model:</p>
<div class="math">$$P(x | y) = \frac{P(y|x)P(x)}{P(y)} = \frac{N(Cx, R) N(0, I)}{N(0, CC^T + R)}$$</div>
<div class="math">$$P(x|y) = N(\beta y, I - \beta C), \ \beta=C^T(CC^T + R)^{-1}$$</div>
<h3>Continuous State</h3>
<p>Here, we presume that the state space is continuous (i.e. x is a continuous variable).</p>
<ol>
<li>(Sensible) Principal Component Analysis (PCA)
PCA is probably one of the most common dimensionality reduction techniques, which at the end of the day (for you lin alg folks) boils down to Singular Value Decomposition (SVD). Here we assume the following:</li>
<li>the observation noise R is a multiple of the identity matrix (i.e. <span class="math">\(R=\alpha I\)</span>)</li>
</ol>
<p>Note that because R is not 0, then we have some noise in the state variables, so this is very similar to probabilistic PCA. </p>
<p>When <span class="math">\(\alpha\)</span> goes to 0, then the noise on the states goes to 0, while Q is still finite. This means that the only noise we assume in our model comes from our observations of the data. A naive approach would simply take the observed covariance matrix of our data, apply SVD to obtain the singular vector matrices and the singular values. The columns of C then are the principle components of PCA. The values of our latent space vector x are the principle values (i.e. singular values of our covariance matrix).</p>
<ol>
<li>Maximum Likelihood Factor Analysis
In factor analysis, we assume the following:</li>
<li>that the observation covariance matrix, R, is diagonal</li>
<li>state noise Q is the identity matrix </li>
</ol>
<p>X are considered the factors, and the key assumption assuming R is diagonal means that the model wants to put all the covariance structure in our observed data (i.e. y variables) into the unique coordinates of R. Note that if R is diagonal, then the off-diagonals (i.e. the covariances) are equal to 0. </p>
<ol>
<li>Summary
In both static modeling procedures, where we assume our latent space and observations are static, we can solve them using EM by using the general inference equations described above for the static model. P(x|y) gives you the inference estimates of states (i.e. x) for a given set of parameters (in this case: C and R). Then for a given set of states (i.e. x), we can maximize wrt the log-likelihood of our model to recover new estimates of C and R. This is the EM algorithm! </li>
</ol>
<p>Quite fascinating that they wrapped this under one umbrella when most people normally look at solving for example PCA using SVD.</p>
<h3>Discrete State</h3>
<p>Here, we presume x is a discrete variable.
1. Gaussian Mixture Models
If x is a discrete probability distribution controlled by the distribution of the noise, w. The mean <span class="math">\(\mu\)</span> and covariance Q parametrize the distribution of x. Now x is modeled by:</p>
<div class="math">$$x = WTA(w)$$</div>
<p>where WTA is the winner take all function applied to the vector w, so x becomes a unit vector of size n. Now the interpretation of x is the mixture weights (i.e. how much each data point y belongs to each cluster). The columns of matrix C represent the cluster means. This is solved via EM also.</p>
<ol>
<li>Vector Quantization (K-means) Models
When the observation noise approaches 0 (i.e. R approaches 0), then we arrive at the k-means algorithm formulation, which can be solved via EM also. So now, P(x|y) is a single point, since there is no noise in the y term, and it is all governed by the noise in the k clusters defined in the state term. </li>
</ol>
<h2>Dynamical Models (i.e. time is a factor)</h2>
<p>Here, we now deal with the fact that time is a factor in our model. So A is no longer the 0 matrix. </p>
<h3>Continuous State - Kalman Filter Models</h3>
<p>The model is generated according to the general model with the noise terms all being independently and identically distributed. This is just solved via the Kalman filter and smoothing algorithms.</p>
<p>If all parameters are known, can employ a Maximum Likelihood approach to estimate the states.</p>
<p>If the parameters are unknown, then EM can be used to iterate on the parameters.</p>
<h3>Discrete State - Hidden Markov Models</h3>
<p>Now, we consider when the states are discrete, which lead to Hidden Markov Models. The model for the states is:</p>
<div class="math">$$x(t+1) = WTA(Ax(t)+w)$$</div>
<p>Here, if we constrain Q to be the identity matrix, then it will have the same covariances for all Gaussians. With traditional Hidden Markov models, we usually define a state transition matrix (e.g. normalized columns), which defines how probable different state transitions are from an initial state. Solving for the most likely state sequence employs the famous Viterbi algorithm. Solving the filtering and smoothing problems, then employ EM traditionally.</p>
<h2>Non-Gaussian Models (i.e. noise terms are no longer normally distributed)</h2>
<p>This results in a famous class of algorithm known as Independent Component Analaysis. This is a generalization of the PCA. Here, we have a nonlinearity applied to the state model:</p>
<div class="math">$$x = g(w)$$</div>
<p>This converts a Gaussian prior (i.e. noise variable w) into a non-Gaussian prior for the state variable x. Now, x is considered our "blind sources" and y is our observed data. In classical ICA, R is assumed to be infinitesimal, and C is square and full-rank. C is considered a "mixing matrix", that is how to mix the sources to "recover" our observed data.</p>
<h2>Control Theory Type Problems</h2>
<p>In control theory and linear systems, we are generally interested in the following problems:</p>
<ul>
<li>Given the general model and outputs measured y, how do we estimate a control input u(t) that controls the dynamics of the state? That is, how do we control the eigenvalues of the system? <ul>
<li>This results in things like state-feedback and the notion of controllability.</li>
</ul>
</li>
<li>Given the general model, how can we estimate the values of the states if we know the rest of the parameters (A, B, C, D)?<ul>
<li>This results in observers and the notion of observability.</li>
</ul>
</li>
<li>Given the general model, how can we estimate an optimal u(t) that follows some constraints and minimizes some cost functional on u? (e.g. constraint on the magnitude of u, or the sparseness of u, etc.)<ul>
<li>This is known as optimal control. It can be deterministic, or stochastic. </li>
</ul>
</li>
</ul>
<p>Note that the Kalman filter/smoothing procedure is an optimal observer under Gaussian noise assumptions. The EM algorithm can be used in general in conjunction with Kalman filter/smoothing to estimate the parameters (A, B, C, D).</p>
<h1>Conclusions</h1>
<p>In this post, I attempt to summarize some of the main points in the Roweis paper that I thought were relevant to someone with knowledge in Linear Algebra, Probability &amp; Statistics and Linear Dynamical Systems. The nice thing is that all these very common algorithms and methods can be framed using Expectation Maximization. This point of view links together control theory, linear dynamical systems and machine learning. It points to how general linear Gaussian models are and how general Expectation Maximization is. </p>
<h3>Dynamic Version of PCA/KMeans?</h3>
<p>What happens if we assume a dynamical model, but instead the output noise is zero (i.e. Q=0)? Here, our states are completely determinable if we have our C matrix. For a linear dynamical system, we can perform PCA to obtain our principle components, which can comprise of our C matrix (i.e. how to go from principle values to observed space). For Hidden Markov Models, we can instead perform vector quantization (i.e. KMeans) to obtain our columns of C. Now, we have to actually estimate the A matrix, which is a first-order Markov dynamic matrix (i.e. governing how states change from time t to t+1). Here it boils down to a simple autoregressive (AR(1)) model in continuous time, or first order Markov chain in the discrete space.</p>
<h1>References:</h1>
<ol>
<li>Roweis S. et al. "A Unifying Review of Linear Gaussian Models".http://mlg.eng.cam.ac.uk/zoubin/papers/lds.pdf</li>
</ol>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
            </div>
            <!-- /.entry-content -->
<section class="well" id="related-posts">
    <h4>Related Posts:</h4>
    <ul>
        <li><a href="/blog/2017/09/simulating-tvb/">Simulating Epileptic iEEG Activity Using The Virtual Brain</a></li>
        <li><a href="/blog/2017/09/fundamental-computational-modeling/">Important Concepts for Computational Modeling</a></li>
        <li><a href="/blog/2017/08/doctoral-board-oral/">Doctoral Board Oral Exam (PhD)</a></li>
        <li><a href="/blog/2017/09/fundamental-papers/">Important Papers for Fundamentals in Computational Neuroscience / Data Science</a></li>
        <li><a href="/blog/2017/12/using-freesurfer/">Using FreeSurfer</a></li>
    </ul>
</section>
        </article>
    </section>

        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>
<!-- Sidebar -->
<section class="well well-sm">
  <ul class="list-group list-group-flush">

<!-- Sidebar/Social -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Social</span></h4>
  <ul class="list-group" id="social">
    <li class="list-group-item"><a href="https://twitter.com/adam2392"><i class="fa fa-twitter-square fa-lg"></i> twitter</a></li>
    <li class="list-group-item"><a href="https://stackexchange.com/users/4494355/ajl123"><i class="fa fa-stack-overflow fa-lg"></i> stack-overflow</a></li>
    <li class="list-group-item"><a href="https://github.com/adam2392"><i class="fa fa-github-square fa-lg"></i> github</a></li>
    <li class="list-group-item"><a href="https://www.linkedin.com/in/adam2392"><i class="fa fa-linkedin-square fa-lg"></i> linkedin</a></li>
  </ul>
</li>
<!-- End Sidebar/Social -->

<!-- Sidebar/Github -->
<li class="list-group-item">
  <h4><i class="fa fa-github fa-lg"></i><span class="icon-label">GitHub Repos</span></h4>
  <div id="gh_repos">
    <p class="list-group-item">Status updating...</p>
  </div>
</li>
<!-- End Sidebar/Github -->
  </ul>
</section>
<!-- End Sidebar -->            </aside>
        </div>
    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2019 Adam Li
            &middot; Powered by <a href="https://github.com/getpelican/pelican-themes/tree/master/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="/theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="/theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="/theme/js/respond.min.js"></script>


<!-- GitHub JS Code -->
<script type="text/javascript">
$(document).ready(function () {
  if (!window.jXHR) {
    var jxhr = document.createElement('script');
    jxhr.type = 'text/javascript';
    jxhr.src = '/theme/js/jXHR.js';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(jxhr, s);
  }

  github.showRepos({
    user: 'adam2392',
    count: 5,
    skip_forks: false,
    target: '#gh_repos'
  });
});
</script>
<script src="/theme/js/github.js" type="text/javascript"></script>
<!-- End GitHub JS Code -->
    <!-- Google Analytics -->
    <script type="text/javascript">

        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-106551801-1']);
        _gaq.push(['_trackPageview']);

        (function () {
            var ga = document.createElement('script');
            ga.type = 'text/javascript';
            ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(ga, s);
        })();
    </script>
    <!-- End Google Analytics Code -->


</body>
</html>